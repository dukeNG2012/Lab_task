         [-21.0658, -16.6612, -17.2124,  ..., -18.8320, -12.6268, -16.2500],
         [-19.7244, -14.7084, -16.4168,  ..., -16.4294, -13.8852, -13.6535],
         [-11.7181, -10.4268, -10.0929,  ..., -11.8916, -13.9373, -10.7553]]],
       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None) and loss_mlm is mlm_output.loss
This is my mrtd_labels: tensor([[-100,    0,    0,    0,    1,    0],
        [-100,    0,    0,    0,    0,    1],
        [-100,    0,    0,    0,    0,    1],
        [-100,    0,    0,    0,    0,    0],
        [-100,    0,    0,    1,    0,    0],
        [-100,    1,    0,    1,    0,    0],
        [-100,    0,    0,    0,    0,    0],
        [-100,    0,    0,    0,    0,    1],
        [-100,    0,    1,    1,    1,    0],
        [-100,    0,    0,    1,    1,    1]], device='cuda:0')
When it is output_MRTD: 


This is output_mrtd: BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.4689, -0.0344, -0.1410,  ...,  0.0301,  0.3944,  0.0498],
         [-0.1098, -0.2179, -0.1661,  ...,  0.3385,  0.3196, -0.3002],
         [-0.1472,  0.0081,  0.1053,  ..., -0.1534,  0.7069, -0.0371],
         [-0.4714, -0.2491, -0.5697,  ...,  0.2934, -0.0573,  1.0311],
         [ 0.8965, -0.0395,  0.1444,  ..., -0.3072,  0.3723, -0.2199],
         [ 1.0060,  0.1057, -0.2438,  ...,  0.2176,  0.4288,  0.2869]],

        [[ 0.2823, -0.3119,  0.0147,  ...,  0.1212,  0.1044, -0.0081],
         [-0.0324, -0.4452, -0.0328,  ...,  0.2117,  0.1895, -0.7279],
         [-0.3279, -0.1023, -0.2846,  ..., -0.0140,  0.4243, -0.2411],
         [-0.0604,  0.1402, -0.3967,  ...,  0.1048,  0.0270,  0.3564],
         [ 0.0851,  0.4854,  0.2793,  ..., -1.0489, -0.0457,  0.0376],
         [ 0.7121,  0.8522,  0.3008,  ..., -0.4121, -0.4347,  0.4389]],

        [[ 0.2803, -0.2954, -0.2112,  ...,  0.2409,  0.0498, -0.1637],
         [-0.3192, -0.0762, -0.2374,  ...,  0.2217,  0.4165, -0.5685],
         [-0.6442, -0.4898, -0.1441,  ...,  0.1407,  0.2478, -0.1844],
         [-0.9282, -0.7984, -0.2193,  ...,  0.0722,  0.1447,  0.6129],
         [-0.5625, -0.0639, -0.3313,  ...,  0.0494,  0.2057,  0.2981],
         [ 0.7886,  0.4971, -0.2515,  ..., -0.6418,  0.7807, -1.2099]],

        ...,

        [[ 0.1060,  0.0923, -0.2608,  ..., -0.0738,  0.1355, -0.1459],
         [-0.4526, -0.0198,  0.4234,  ...,  0.1717, -0.1364, -0.0172],
         [-0.9461, -0.8253, -0.2320,  ..., -0.1103,  0.4248, -0.0914],
         [-0.1281, -0.0584, -0.5390,  ...,  0.3206, -0.1404,  0.9909],
         [ 0.1506, -0.9072, -0.0905,  ..., -0.2216,  0.2228,  0.3800],
         [ 0.9106,  0.2143, -0.2636,  ...,  0.1314, -0.2708,  0.4289]],

        [[ 0.8185,  0.7442, -0.4807,  ...,  0.0847,  0.7409, -0.0809],
         [ 0.0021,  0.2220, -0.1033,  ...,  0.3677,  0.4092, -1.1501],
         [ 0.4180,  0.5818, -0.5182,  ...,  0.1022,  0.7593, -1.1916],
         [ 0.1218,  0.5684, -0.3332,  ...,  0.2597,  0.6985, -0.3213],
         [ 0.6029, -0.2021, -0.4316,  ...,  0.7323,  0.6256, -0.4843],
         [ 0.4464,  0.3205, -0.3894,  ..., -0.0764,  0.2309,  0.0379]],

        [[ 0.1030, -0.1106, -0.1088,  ...,  0.3014,  0.5287,  0.1328],
         [-0.8113, -0.0964, -0.1441,  ...,  0.5871, -0.0725, -0.3675],
         [-0.5408,  0.4211,  0.1092,  ...,  0.7599,  0.2745, -0.1751],
         [-0.8227, -0.0591, -1.1024,  ...,  0.3758,  0.5170,  0.9907],
         [-0.4297,  1.2461, -0.0120,  ...,  0.6738,  0.4769,  0.1222],
         [ 0.1601,  0.3995, -0.5052,  ...,  0.5922,  0.7469,  0.4030]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=None, hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None) 

This is my text1: {'input_ids': tensor([[ 101, 1996, 2158, 2003, 3061, 2007],
        [ 101, 1996, 2450, 2038, 2146, 2601],
        [ 101, 1996, 2611, 2003, 4147, 2014],
        [ 101, 2023, 2450, 2003, 4147, 1037],
        [ 101, 1037, 2158, 4147, 1037, 2317],
        [ 101, 1037, 2711, 2003, 3788, 2185],
        [ 101, 1996, 2158, 2003, 4147, 1037],
        [ 101, 2023, 2711, 2038, 2460, 2601],
        [ 101, 1996, 2711, 2003, 4147, 1037],
        [ 101, 1996, 2450, 2038, 2601, 2606]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1]], device='cuda:0')} 

This is my text1.attention_mask: tensor([[1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1]], device='cuda:0') 

This is my image_atts: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0') 

This is my  self.text_encoder.config.vocab_size: 30522
This is input_ids: tensor([[  101,  1996,   103,  2003,  4147,  1037],
        [  101,  2023,  2450,  2003,  4147,  1037],
        [  101,  2016,  2038,  2460,  2304,  2606],
        [  101,   103,  2007,  1037,   103,  3797],
        [  101,  1037,  2450,  4147,  1037,  2304],
        [  101,  1037,  2450,  2003,  3061,  2007],
        [  101,  2002,  2038,   103, 10277,  2304],
        [  101,   103,  2158,  2003,  4147,  1037],
        [  101,  2016,  2003,  4147,  2304,  6007],
        [  101,  1996,  2158, 11651,  1037,  2304]], device='cuda:0')


This is targets=labels: tensor([[ -100,  -100,  2158,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100],
        [ -100,  2158,  -100,  -100,  5061,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100, 19957,  -100,  -100],
        [ -100,  2402,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100]], device='cuda:0')
This is probability matrix: tensor([[0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500]])
When it is mlm output: 


This is mlm_output: MaskedLMOutput(loss=tensor(4.2417, device='cuda:0', grad_fn=<AddBackward0>), logits=tensor([[[-24.6404, -15.7604, -17.6471,  ..., -20.3192, -15.2623, -15.8121],
         [-19.8887, -15.4739, -14.4907,  ..., -14.9717, -12.2999, -14.3042],
         [-14.8065, -11.0795, -10.1685,  ..., -11.5500, -11.9562, -11.7087],
         [-23.3724, -16.6957, -17.2981,  ..., -18.6772, -15.5959, -17.6593],
         [-20.7784, -13.5888, -17.4202,  ..., -17.1146, -14.9995, -16.7415],
         [-17.4337, -14.6412, -15.2212,  ..., -15.6278, -15.2758, -13.7972]],

        [[-25.5056, -15.7644, -15.0233,  ..., -19.6289, -16.1127, -17.5512],
         [-21.9688, -15.5453, -13.5308,  ..., -16.4270, -14.5859, -12.1034],
         [-20.6932, -16.0097, -14.1340,  ..., -15.7921, -16.2298, -16.6848],
         [-23.8178, -18.2884, -18.5425,  ..., -19.7898, -15.0926, -16.9659],
         [-19.1023, -12.9151, -15.4086,  ..., -15.8395, -14.0386, -13.8661],
         [-15.9919, -14.4516, -15.3823,  ..., -15.7568, -15.7063, -13.0186]],

        [[-21.2644, -17.2558, -16.4592,  ..., -20.5768, -15.5452, -18.7477],
         [-18.2684, -14.2814, -13.5040,  ..., -14.3579, -13.9499, -12.4124],
         [-18.4649, -15.7702, -16.0672,  ..., -15.8819, -13.0651, -14.7739],
         [-19.2595, -15.9524, -17.3223,  ..., -18.2054, -12.6707, -12.7646],
         [-20.3903, -15.8817, -15.9190,  ..., -16.3821, -15.7593, -12.4639],
         [-20.1950, -15.7376, -17.3753,  ..., -18.6269, -17.6518, -14.3383]],

        ...,

        [[-26.2330, -16.7961, -18.4443,  ..., -19.5871, -15.4551, -17.9742],
         [-16.2711, -11.6468, -11.4850,  ..., -11.4524, -12.5296, -10.7969],
         [-21.9006, -15.2428, -15.8432,  ..., -17.9135, -16.7148, -19.2327],
         [-24.5813, -15.2144, -16.9516,  ..., -18.7824, -15.1500, -17.0939],
         [-20.2792, -12.8365, -16.3584,  ..., -16.3983, -14.2149, -15.6188],
         [-17.9946, -15.7026, -15.6996,  ..., -17.1133, -15.4167, -14.5440]],

        [[-25.1630, -15.3615, -15.2156,  ..., -17.6877, -14.3237, -14.4424],
         [-18.7334, -13.2396, -12.9968,  ..., -13.3479, -14.3742, -10.5989],
         [-21.4756, -15.1585, -15.9915,  ..., -16.1815, -12.7933, -12.5254],
         [-19.6908, -14.4257, -16.7224,  ..., -16.3180, -13.8808, -14.9755],
         [-23.2231, -15.8122, -16.1090,  ..., -16.7018, -14.5216, -12.4040],
         [-17.4951, -14.9110, -16.6562,  ..., -14.1403, -15.5371, -14.6463]],

        [[-25.5346, -14.9075, -16.2395,  ..., -17.2425, -13.8594, -16.7428],
         [-18.9833, -13.3471, -14.6397,  ..., -13.0631, -11.2790, -12.0719],
         [-20.7926, -15.4186, -16.3687,  ..., -18.2192, -15.8392, -19.0111],
         [-15.1202, -15.1937, -16.2365,  ..., -17.2979, -15.1312, -15.9432],
         [-22.1018, -16.7920, -18.2592,  ..., -19.1410, -14.9544, -16.8381],
         [-21.1341, -14.2400, -15.5469,  ..., -14.9667, -16.8399, -15.8798]]],
       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None) 

This is my text1: {'input_ids': tensor([[  101,  1996,  2158,  2003,  4147,  1037],
        [  101,  2023,  2450,  2003,  4147,  1037],
        [  101,  2016,  2038,  2460,  2304,  2606],
        [  101,  2158,  2007,  1037,  5061,  3797],
        [  101,  1037,  2450,  4147,  1037,  2304],
        [  101,  1037,  2450,  2003,  3061,  2007],
        [  101,  2002,  2038, 19957, 10277,  2304],
        [  101,  2402,  2158,  2003,  4147,  1037],
        [  101,  2016,  2003,  4147,  2304,  6007],
        [  101,  1996,  2158, 11651,  1037,  2304]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1]], device='cuda:0')} 

This is my text1.attention_mask: tensor([[1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1]], device='cuda:0') 

This is my image_embeds: tensor([[[ 1.5231e-01,  1.1772e-01,  5.4234e-02,  ..., -4.2013e-03,
          -2.4440e-02,  1.2820e-01],
         [ 2.0060e-01,  3.1441e-02,  5.2401e-02,  ..., -1.9506e-03,
           3.1752e-04,  2.2616e-01],
         [ 1.7364e-01,  1.9121e-02,  3.6062e-02,  ...,  3.1544e-02,
          -2.3716e-02,  2.2085e-01],
         ...,
         [ 4.8406e-02, -1.7381e-01,  1.5168e-01,  ...,  1.9778e-02,
           1.6056e-01,  1.2781e-01],
         [ 3.1534e-01,  6.5943e-02,  2.7986e-01,  ..., -1.5777e-01,
           1.3678e-01,  1.0127e-01],
         [ 1.7250e-01,  4.8046e-02,  5.8829e-02,  ...,  1.8695e-02,
           2.8471e-03,  1.8218e-01]],

        [[ 1.3812e-01,  8.6465e-02,  3.6571e-02,  ..., -1.3645e-03,
          -3.4700e-02,  1.3630e-01],
         [ 1.4747e-02, -9.1815e-01,  1.7941e-01,  ..., -9.5734e-02,
           8.0255e-01, -4.0367e-01],
         [ 1.9081e-01,  2.4339e-02,  4.2827e-02,  ...,  2.7994e-02,
          -3.6976e-02,  1.8602e-01],
         ...,
         [ 2.2837e-01,  1.1597e-02,  1.6172e-01,  ..., -4.4590e-02,
           5.7637e-02,  1.5868e-01],
         [ 3.4730e-01, -3.5344e-01, -5.0883e-02,  ...,  6.4196e-02,
           1.5733e-01,  5.2344e-01],
         [ 1.9166e-01,  1.8230e-01,  1.2549e-01,  ..., -3.6721e-02,
          -9.9858e-03,  1.8845e-01]],

        [[ 1.4538e-01,  1.0011e-01,  7.5974e-03,  ..., -1.3764e-02,
          -2.3780e-02,  1.4112e-01],
         [ 3.9427e-02, -9.8052e-02,  1.2428e-01,  ...,  2.0261e-02,
           2.6187e-02,  4.3646e-01],
         [ 1.4124e-01,  6.3738e-03,  8.0087e-02,  ...,  1.6161e-02,
           1.8947e-02,  1.7867e-01],
         ...,
         [ 8.8557e-01, -5.6759e-01, -5.6010e-03,  ..., -1.2374e-01,
           8.4550e-01,  1.0150e-01],
         [ 2.0541e-01,  5.8500e-02,  7.8734e-02,  ..., -5.0355e-02,
          -2.3703e-02,  1.5557e-01],
         [ 3.7505e-01, -2.4849e-01, -3.3847e-01,  ..., -2.1167e-02,
           3.5846e-01,  5.1158e-01]],

        ...,

        [[ 1.5921e-01,  8.6787e-02,  3.4215e-02,  ..., -1.6012e-02,
          -1.8617e-02,  1.3194e-01],
         [ 1.6325e-01,  4.9660e-02,  9.4353e-02,  ...,  2.9411e-02,
           6.4417e-02,  1.6852e-01],
         [ 1.7986e-01,  6.1297e-02,  1.2156e-01,  ..., -5.2844e-02,
           5.1303e-02,  1.3719e-01],
         ...,
         [ 1.7659e-01,  4.0123e-02,  1.0148e-01,  ..., -1.1367e-01,
           1.1331e-01,  2.1539e-01],
         [ 4.2658e-01, -2.1419e-01,  2.5430e-01,  ..., -1.1039e-01,
           1.1718e-01,  3.1419e-01],
         [-2.8860e-01, -6.6694e-01, -2.0383e-01,  ...,  3.2034e-01,
           1.1545e+00, -1.5410e+00]],

        [[ 1.2820e-01,  9.6866e-02,  4.4161e-02,  ..., -8.4982e-03,
          -1.6102e-02,  1.2544e-01],
         [ 1.3768e-01,  4.3218e-03,  1.2704e-01,  ...,  7.0607e-02,
           3.9556e-02,  1.6316e-01],
         [ 1.2209e-01,  1.4153e-01,  1.5749e-01,  ..., -1.4261e-01,
           5.5037e-02,  8.2828e-02],
         ...,
         [ 1.3225e-01,  5.6213e-02,  1.1340e-01,  ..., -3.1152e-02,
           7.8345e-02,  1.4394e-01],
         [ 1.6835e-01,  3.8111e-02,  8.7422e-02,  ..., -1.3574e-03,
           4.0249e-02,  1.4165e-01],
         [ 1.3894e-01,  9.5579e-02,  7.7086e-02,  ..., -1.8220e-01,
           1.5191e-01,  1.8643e-01]],

        [[ 1.0491e-01,  1.1925e-01,  1.8321e-02,  ..., -6.5313e-03,
          -4.5412e-02,  1.6620e-01],
         [ 1.1375e-01,  4.8981e-02,  1.0491e-01,  ...,  1.9641e-03,
          -2.9360e-02,  2.0965e-01],
         [-7.8017e-02, -6.0236e-01,  1.0005e+00,  ..., -8.2032e-01,
           8.3784e-01,  2.0163e-01],
         ...,
         [ 1.3656e-01, -8.1548e-01,  1.4304e-01,  ..., -7.0658e-01,
           2.2870e-01, -3.3535e-01],
         [ 5.9667e-02, -1.2094e+00,  2.9435e-01,  ..., -2.9135e-01,
           9.7079e-01, -2.6103e-01],
         [ 3.6291e-02,  1.1108e-01,  2.0931e-02,  ..., -4.4514e-02,
          -4.9042e-02,  2.3073e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
This is my image_atts: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0') 

This is my labels: tensor([[ -100,  -100,  2158,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100],
        [ -100,  2158,  -100,  -100,  5061,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100, 19957,  -100,  -100],
        [ -100,  2402,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100]], device='cuda:0')
This is my prediction: tensor([[[2.8005e-11, 1.0268e-07, 1.7252e-08,  ..., 8.4408e-09,
          1.4012e-07, 8.2820e-08],
         [2.2132e-11, 4.5374e-10, 5.6750e-10,  ..., 4.7368e-10,
          6.8188e-09, 1.1453e-09],
         [1.0678e-11, 1.8515e-10, 3.8891e-10,  ..., 8.7740e-11,
          6.0480e-11, 6.7899e-11],
         [1.0100e-13, 1.2310e-10, 5.0573e-11,  ..., 2.1341e-11,
          7.7047e-11, 4.0326e-11],
         [5.5110e-13, 5.4074e-10, 3.9135e-12,  ..., 3.2336e-11,
          4.6131e-11, 1.8901e-11],
         [3.5736e-10, 1.1620e-09, 5.8637e-10,  ..., 4.8725e-10,
          5.6548e-10, 1.9144e-09]],

        [[1.2679e-11, 6.6096e-07, 1.7726e-07,  ..., 1.9642e-08,
          3.7954e-08, 1.0206e-07],
         [8.7606e-14, 1.3326e-11, 3.5897e-11,  ..., 9.4159e-12,
          3.3796e-11, 5.2237e-11],
         [1.0879e-13, 1.8229e-11, 7.7460e-11,  ..., 2.6753e-11,
          1.9723e-11, 1.0015e-11],
         [1.2287e-14, 9.2505e-11, 1.4854e-11,  ..., 4.9264e-12,
          2.6837e-10, 4.5348e-11],
         [7.8945e-13, 7.3331e-11, 2.1887e-12,  ..., 4.3463e-12,
          4.3213e-11, 2.1565e-12],
         [2.7083e-11, 1.2548e-10, 9.1521e-11,  ..., 3.3924e-11,
          8.4346e-11, 3.5604e-10]],

        [[1.6988e-11, 1.7188e-08, 2.1867e-08,  ..., 1.5020e-09,
          4.9176e-09, 2.7820e-09],
         [4.6265e-13, 8.0767e-11, 2.0003e-10,  ..., 1.2390e-10,
          5.2419e-10, 2.6750e-10],
         [6.7769e-13, 2.3752e-11, 9.0294e-11,  ..., 2.2008e-11,
          5.4112e-10, 2.7848e-11],
         [1.5350e-12, 2.5775e-11, 2.1931e-11,  ..., 2.7757e-12,
          1.6866e-10, 1.1811e-10],
         [7.1677e-13, 6.2851e-11, 2.9070e-10,  ..., 7.8478e-11,
          7.1243e-11, 6.2790e-10],
         [1.4200e-12, 5.5976e-11, 8.0055e-12,  ..., 3.8606e-12,
          3.7946e-12, 7.5518e-11]],

        ...,

        [[2.8701e-11, 6.3289e-07, 4.4450e-08,  ..., 3.6036e-08,
          1.5602e-06, 5.4595e-08],
         [1.9860e-12, 4.8123e-11, 6.4937e-11,  ..., 1.8429e-10,
          1.4795e-10, 1.6974e-10],
         [4.1769e-13, 2.5282e-10, 4.6344e-11,  ..., 6.0966e-12,
          7.0709e-11, 9.7270e-13],
         [6.9642e-14, 1.5565e-10, 3.5029e-11,  ..., 1.2091e-11,
          2.4284e-10, 1.4090e-10],
         [1.4663e-13, 1.0462e-10, 3.6034e-12,  ..., 5.8316e-12,
          1.9851e-11, 2.7548e-12],
         [7.0681e-11, 2.5482e-10, 1.1849e-10,  ..., 1.8038e-10,
          1.0355e-10, 3.0773e-10]],

        [[3.8766e-11, 6.7238e-09, 4.4580e-09,  ..., 3.8823e-09,
          1.0672e-08, 6.3737e-08],
         [7.1928e-13, 2.4557e-11, 6.5621e-11,  ..., 2.5272e-10,
          1.1504e-10, 9.8283e-11],
         [5.7919e-13, 2.6688e-09, 1.4157e-10,  ..., 4.5902e-10,
          5.9480e-09, 7.8182e-10],
         [5.9861e-12, 1.4659e-08, 5.9366e-10,  ..., 1.6629e-09,
          1.6502e-08, 5.9868e-10],
         [2.2646e-14, 1.8439e-11, 1.8657e-11,  ..., 2.9558e-11,
          4.5137e-11, 1.7669e-10],
         [3.3763e-11, 3.6325e-10, 3.8819e-10,  ..., 1.7626e-09,
          4.7669e-10, 1.8224e-10]],

        [[1.6304e-11, 8.2351e-07, 1.1807e-07,  ..., 4.2809e-08,
          2.4212e-07, 6.0261e-08],
         [7.1101e-12, 1.6670e-10, 1.7899e-10,  ..., 2.0982e-10,
          3.8779e-09, 7.3154e-11],
         [2.1279e-12, 2.0632e-11, 2.3029e-11,  ..., 4.6333e-12,
          1.6355e-11, 1.1662e-12],
         [1.2957e-11, 1.4267e-11, 7.9982e-12,  ..., 9.5445e-13,
          1.7312e-10, 2.4898e-12],
         [3.9722e-13, 3.0415e-11, 4.8916e-11,  ..., 9.2673e-12,
          1.1104e-09, 2.8193e-11],
         [3.1081e-12, 2.3826e-10, 5.1705e-11,  ..., 1.8150e-10,
          9.0641e-11, 9.2331e-11]]], device='cuda:0')
This is my alpha: 0.0002348796241926013
This is my mlm_output: MaskedLMOutput(loss=tensor(4.2417, device='cuda:0', grad_fn=<AddBackward0>), logits=tensor([[[-24.6404, -15.7604, -17.6471,  ..., -20.3192, -15.2623, -15.8121],
         [-19.8887, -15.4739, -14.4907,  ..., -14.9717, -12.2999, -14.3042],
         [-14.8065, -11.0795, -10.1685,  ..., -11.5500, -11.9562, -11.7087],
         [-23.3724, -16.6957, -17.2981,  ..., -18.6772, -15.5959, -17.6593],
         [-20.7784, -13.5888, -17.4202,  ..., -17.1146, -14.9995, -16.7415],
         [-17.4337, -14.6412, -15.2212,  ..., -15.6278, -15.2758, -13.7972]],

        [[-25.5056, -15.7644, -15.0233,  ..., -19.6289, -16.1127, -17.5512],
         [-21.9688, -15.5453, -13.5308,  ..., -16.4270, -14.5859, -12.1034],
         [-20.6932, -16.0097, -14.1340,  ..., -15.7921, -16.2298, -16.6848],
         [-23.8178, -18.2884, -18.5425,  ..., -19.7898, -15.0926, -16.9659],
         [-19.1023, -12.9151, -15.4086,  ..., -15.8395, -14.0386, -13.8661],
         [-15.9919, -14.4516, -15.3823,  ..., -15.7568, -15.7063, -13.0186]],

        [[-21.2644, -17.2558, -16.4592,  ..., -20.5768, -15.5452, -18.7477],
         [-18.2684, -14.2814, -13.5040,  ..., -14.3579, -13.9499, -12.4124],
         [-18.4649, -15.7702, -16.0672,  ..., -15.8819, -13.0651, -14.7739],
         [-19.2595, -15.9524, -17.3223,  ..., -18.2054, -12.6707, -12.7646],
         [-20.3903, -15.8817, -15.9190,  ..., -16.3821, -15.7593, -12.4639],
         [-20.1950, -15.7376, -17.3753,  ..., -18.6269, -17.6518, -14.3383]],

        ...,

        [[-26.2330, -16.7961, -18.4443,  ..., -19.5871, -15.4551, -17.9742],
         [-16.2711, -11.6468, -11.4850,  ..., -11.4524, -12.5296, -10.7969],
         [-21.9006, -15.2428, -15.8432,  ..., -17.9135, -16.7148, -19.2327],
         [-24.5813, -15.2144, -16.9516,  ..., -18.7824, -15.1500, -17.0939],
         [-20.2792, -12.8365, -16.3584,  ..., -16.3983, -14.2149, -15.6188],
         [-17.9946, -15.7026, -15.6996,  ..., -17.1133, -15.4167, -14.5440]],

        [[-25.1630, -15.3615, -15.2156,  ..., -17.6877, -14.3237, -14.4424],
         [-18.7334, -13.2396, -12.9968,  ..., -13.3479, -14.3742, -10.5989],
         [-21.4756, -15.1585, -15.9915,  ..., -16.1815, -12.7933, -12.5254],
         [-19.6908, -14.4257, -16.7224,  ..., -16.3180, -13.8808, -14.9755],
         [-23.2231, -15.8122, -16.1090,  ..., -16.7018, -14.5216, -12.4040],
         [-17.4951, -14.9110, -16.6562,  ..., -14.1403, -15.5371, -14.6463]],

        [[-25.5346, -14.9075, -16.2395,  ..., -17.2425, -13.8594, -16.7428],
         [-18.9833, -13.3471, -14.6397,  ..., -13.0631, -11.2790, -12.0719],
         [-20.7926, -15.4186, -16.3687,  ..., -18.2192, -15.8392, -19.0111],
         [-15.1202, -15.1937, -16.2365,  ..., -17.2979, -15.1312, -15.9432],
         [-22.1018, -16.7920, -18.2592,  ..., -19.1410, -14.9544, -16.8381],
         [-21.1341, -14.2400, -15.5469,  ..., -14.9667, -16.8399, -15.8798]]],
       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None) and loss_mlm is mlm_output.loss
This is my mrtd_labels: tensor([[-100,    0,    0,    0,    1,    0],
        [-100,    1,    0,    0,    0,    0],
        [-100,    1,    0,    0,    1,    0],
        [-100,    0,    1,    0,    1,    1],
        [-100,    0,    0,    0,    0,    0],
        [-100,    1,    0,    0,    0,    1],
        [-100,    1,    0,    1,    0,    0],
        [-100,    0,    0,    0,    0,    1],
        [-100,    1,    1,    1,    0,    0],
        [-100,    1,    1,    0,    0,    0]], device='cuda:0')
When it is output_MRTD: 


This is output_mrtd: BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2189, -0.3551, -0.2601,  ...,  0.0611,  0.2570,  0.0536],
         [-0.3401,  0.1087, -0.1445,  ...,  0.0908,  0.1030, -0.4047],
         [-0.3888,  0.3441,  0.0504,  ..., -0.0068,  0.7878,  0.1523],
         [-0.7658, -0.0900, -0.2192,  ...,  0.2891, -0.3242,  0.9624],
         [-0.1030, -0.4066,  0.1346,  ...,  0.1957,  0.4250,  0.2425],
         [-0.2112, -0.2627, -0.4715,  ..., -0.0849,  0.1760,  0.2617]],

        [[ 0.2156, -0.1651, -0.3025,  ...,  0.0422,  0.3674,  0.1195],
         [-0.4775, -0.3575, -0.1899,  ...,  0.2362,  0.2705, -0.5813],
         [-0.5277,  0.0875, -0.4181,  ...,  0.0141,  0.3808, -0.1895],
         [-1.1825, -0.5076, -0.0408,  ...,  0.5149,  0.2158,  0.6732],
         [-0.0432, -0.3264, -0.1230,  ...,  0.4411,  0.1340,  0.4494],
         [-0.1838, -0.3281, -0.8327,  ...,  0.1395, -0.0389,  0.6542]],

        [[ 0.4091, -0.2346, -0.3429,  ..., -0.0257,  0.1033, -0.0794],
         [-0.1786,  0.4765, -0.0527,  ...,  0.0373,  0.6350, -0.2641],
         [ 0.3825,  0.3211, -0.3429,  ...,  0.1859,  0.2269,  0.1691],
         [ 0.4879, -0.0847, -0.3536,  ...,  0.1284,  0.0277, -0.1148],
         [ 0.3328,  0.0418, -0.0427,  ...,  0.1070,  0.0177, -0.5092],
         [ 0.6379,  0.1129,  0.2037,  ..., -0.0153, -0.0644, -0.2660]],

        ...,

        [[ 0.1835, -0.0335, -0.3111,  ..., -0.0071,  0.2503, -0.1462],
         [-0.0061,  0.2103,  0.1300,  ..., -0.3458,  0.1328, -1.7101],
         [-0.2277,  0.2464, -0.2458,  ...,  0.0902,  0.5310, -0.4873],
         [-0.7168, -0.0161, -0.1608,  ...,  0.2435,  0.3498,  0.5904],
         [-0.3360, -0.2570,  0.1681,  ...,  0.0094,  0.1655,  0.3998],
         [-0.6040,  0.9492, -0.2689,  ..., -0.1854,  0.2452,  0.3013]],

        [[ 0.0452, -0.0308, -0.0269,  ..., -0.0474,  0.1652, -0.1130],
         [-0.8880,  0.1866, -0.4433,  ..., -0.0533,  0.1404, -0.5038],
         [-0.3007, -0.0714,  0.1293,  ...,  0.1169,  0.1663, -0.1924],
         [ 0.9092,  0.1359, -0.2065,  ..., -0.3350,  0.1712,  0.1043],
         [ 0.8060, -0.2252, -0.0554,  ..., -0.4569,  0.5398, -0.5378],
         [ 0.6947,  0.6197,  0.1654,  ...,  0.0806,  0.2666, -1.1028]],

        [[ 0.4911,  0.0566,  0.2160,  ...,  0.0380,  0.2259,  0.0155],
         [ 0.3561,  0.1580,  0.2546,  ...,  0.4499,  0.2657,  0.0596],
         [ 0.4998, -0.4872,  0.0990,  ...,  0.0866,  0.7100, -0.4102],
         [-0.2089,  0.4671,  0.2110,  ...,  0.6505,  0.4701, -0.3012],
         [-0.1344, -0.2483,  0.1502,  ...,  0.0404, -0.1106,  0.1125],
         [ 0.5558,  0.0273,  0.2865,  ...,  0.0401,  0.3860, -0.7259]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=None, hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None) 

This is my text1: {'input_ids': tensor([[  101,  1996,  2158,  2003,  4147,  1037],
        [  101,  2023,  2450,  2003,  4147,  1037],
        [  101,  2016,  2038,  2460,  2304,  2606],
        [  101,  2158,  2007,  1037,  5061,  3797],
        [  101,  1037,  2450,  4147,  1037,  2304],
        [  101,  1037,  2450,  2003,  3061,  2007],
        [  101,  2002,  2038, 19957, 10277,  2304],
        [  101,  2402,  2158,  2003,  4147,  1037],
        [  101,  2016,  2003,  4147,  2304,  6007],
        [  101,  1996,  2158, 11651,  1037,  2304]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1]], device='cuda:0')} 

This is my text1.attention_mask: tensor([[1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1]], device='cuda:0') 

This is my image_atts: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0') 

This is my  self.text_encoder.config.vocab_size: 30522
This is input_ids: tensor([[  101,  1996,  2158,  2003,  5102,  1999,     0,     0],
        [  101,  1996,  2158,  2003,  4147,  1037,     0,     0],
        [  101,  1996,  2611, 11651,  2422,  6910,     0,     0],
        [  101,  1037,   103,  4147,  1037,  2417,     0,     0],
        [  101,  1037,  2146,  2601, 10681,  2450,     0,     0],
        [  101,  1996,  2879,   103,  2460,  2304,     0,     0],
        [  101,   103,  2450,  2003,  4147,  1037,     0,     0],
        [  101,  2023,  2711,   103,  5710,  2013,     0,     0],
        [  101,  1037,  2450,  4147,  1037,   103,     0,     0],
        [  101,  2023,  2402,  2711,  2038,   103, 15621,   103]],
       device='cuda:0')


This is targets=labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100],
        [-100, -100, 2158, -100, -100, -100, -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100],
        [-100, -100, -100, 2038, -100, 2304, -100, -100],
        [-100, 2023, -100, -100, -100, -100, -100, -100],
        [-100, -100, -100, 2003, -100, -100, -100, -100],
        [-100, -100, -100, -100, -100, 6379, -100, -100],
        [-100, -100, -100, -100, -100, 3239, -100, 2229]], device='cuda:0')
This is probability matrix: tensor([[0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500],
        [0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500]])
When it is mlm output: 


This is mlm_output: MaskedLMOutput(loss=tensor(1.9004, device='cuda:0', grad_fn=<AddBackward0>), logits=tensor([[[-26.1673, -16.3332, -17.4221,  ..., -19.3560, -16.8200, -18.2709],
         [-20.9237, -14.3162, -13.9621,  ..., -14.4302, -12.2276, -13.4404],
         [-20.1239, -14.7750, -14.3859,  ..., -16.7994, -15.1163, -17.6509],
         ...,
         [-16.7680, -14.8951, -15.4985,  ..., -17.4272, -13.8230, -13.9900],
         [-10.6318, -10.4140,  -9.3026,  ...,  -8.9186, -11.9693,  -9.8684],
         [-25.3275, -15.9572, -16.4141,  ..., -18.3555, -14.7294, -16.9521]],

        [[-21.6656, -16.8204, -18.1557,  ..., -21.2703, -16.7858, -19.4321],
         [-20.3143, -15.6962, -15.7424,  ..., -15.3246, -12.3142, -13.7507],
         [-21.5261, -14.4518, -14.7610,  ..., -17.1346, -16.0138, -17.7662],
         ...,
         [-18.8083, -14.5101, -16.3719,  ..., -16.1109, -13.1954, -14.0117],
         [-12.2403, -10.6600,  -9.3704,  ...,  -9.2633, -11.6299, -11.5084],
         [-13.1598, -10.1198,  -9.5922,  ...,  -9.5071, -11.5628, -11.6133]],

        [[-26.0633, -14.5706, -16.6168,  ..., -19.2920, -14.5024, -15.6515],
         [-19.3284, -13.5762, -13.8121,  ..., -14.4926, -10.9700, -11.7273],
         [-20.1521, -14.9063, -13.2215,  ..., -17.8103, -13.2084, -13.6204],
         ...,
         [-14.2684, -13.6765, -13.0919,  ..., -13.0705, -14.1121, -11.7202],
         [-25.1853, -14.3007, -15.3469,  ..., -18.1034, -14.4486, -16.1055],
         [-10.8624,  -9.4720, -10.0111,  ..., -10.1869, -12.3075, -10.3400]],

        ...,

        [[-25.1141, -18.9883, -18.1642,  ..., -21.2565, -17.4835, -21.0115],
         [-18.4311, -17.1106, -14.7104,  ..., -16.5812, -15.3390, -14.8835],
         [-23.6262, -14.9346, -15.3868,  ..., -16.9535, -13.6797, -15.8394],
         ...,
         [-16.6692, -16.5970, -16.1377,  ..., -15.7024, -13.9890, -14.5407],
         [-10.5619, -11.1887, -10.1297,  ..., -10.8610, -12.8977, -11.9899],
         [ -9.6211, -11.2431, -10.2629,  ..., -10.5360, -12.7048, -12.3279]],

        [[-25.9070, -15.5251, -16.2128,  ..., -19.4814, -15.4794, -16.0620],
         [-23.0490, -16.8681, -18.1329,  ..., -19.3838, -14.1550, -16.1042],
         [-21.9720, -14.0651, -13.5556,  ..., -14.9907, -15.7606, -15.5245],
         ...,
         [-12.8301,  -9.7949,  -9.3522,  ...,  -8.8506, -12.1869, -11.5863],
         [-13.1451, -10.0017, -10.2098,  ...,  -9.5770, -12.3338, -11.6338],
         [-12.8776,  -9.5350,  -8.9441,  ...,  -9.1539, -11.8358, -11.0125]],

        [[-24.9737, -19.1755, -18.4713,  ..., -20.6228, -15.1489, -19.3756],
         [-23.2183, -17.1531, -15.3044,  ..., -18.0688, -14.5765, -15.6702],
         [-21.3140, -15.4276, -15.6206,  ..., -17.7378, -14.3985, -17.0800],
         ...,
         [-13.3938, -10.7807, -10.2818,  ..., -10.9723, -10.3980, -10.0745],
         [-13.2841, -13.9886, -13.1125,  ..., -13.2658,  -9.2197, -12.1348],
         [-11.0311, -11.3117, -11.7773,  ..., -11.3091, -11.3865, -11.8826]]],
       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None) 

This is my text1: {'input_ids': tensor([[  101,  1996,  2158,  2003,  5102,  1999,     0,     0],
        [  101,  1996,  2158,  2003,  4147,  1037,     0,     0],
        [  101,  1996,  2611, 11651,  2422,  6910,     0,     0],
        [  101,  1037,  2158,  4147,  1037,  2417,     0,     0],
        [  101,  1037,  2146,  2601, 10681,  2450,     0,     0],
        [  101,  1996,  2879,  2038,  2460,  2304,     0,     0],
        [  101,  2023,  2450,  2003,  4147,  1037,     0,     0],
        [  101,  2023,  2711,  2003,  5710,  2013,     0,     0],
        [  101,  1037,  2450,  4147,  1037,  6379,     0,     0],
        [  101,  2023,  2402,  2711,  2038,  3239, 15621,  2229]],
       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')} 

This is my text1.attention_mask: tensor([[1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0') 

This is my image_embeds: tensor([[[ 1.4636e-01,  1.0027e-01,  4.4815e-02,  ..., -6.8356e-04,
          -1.2386e-02,  1.2687e-01],
         [ 1.2793e-01,  7.6224e-02,  1.0901e-01,  ..., -2.1248e-03,
           1.1739e-02,  1.9658e-01],
         [ 1.9926e-01,  5.1303e-02,  8.6510e-02,  ...,  6.2190e-03,
           1.5395e-02,  1.9060e-01],
         ...,
         [ 2.0642e-01,  2.0210e-02,  8.1864e-02,  ...,  3.3350e-02,
           1.1086e-02,  1.2378e-01],
         [ 1.9376e-01,  4.3208e-02,  7.2017e-02,  ...,  1.6420e-04,
           4.2903e-04,  1.5058e-01],
         [ 1.4721e-01,  5.4176e-02,  5.8791e-02,  ..., -1.1455e-02,
          -1.3586e-02,  2.1977e-01]],

        [[ 1.2839e-01,  9.3962e-02,  4.2161e-02,  ...,  1.1970e-02,
          -3.8314e-02,  1.4573e-01],
         [ 1.7469e-01,  2.1178e-02,  6.7194e-02,  ..., -7.5793e-02,
           8.0765e-02,  2.5570e-01],
         [ 1.3669e-01,  1.1527e-02,  8.0448e-03,  ..., -3.6966e-02,
           1.0837e-03,  1.5611e-01],
         ...,
         [ 1.3303e-01,  3.7854e-02,  8.6636e-02,  ..., -3.9688e-02,
           2.6224e-04,  1.8997e-01],
         [ 1.5161e-01,  5.8682e-02,  4.7494e-02,  ..., -4.7263e-02,
          -1.0292e-02,  2.3875e-01],
         [ 2.5739e-01, -4.5215e-01,  3.0363e-01,  ..., -2.1110e-01,
           6.4169e-01,  5.3024e-01]],

        [[ 1.4257e-01,  1.0367e-01,  4.9626e-02,  ..., -1.4055e-03,
          -2.4368e-02,  1.4010e-01],
         [ 1.6550e-01,  5.2351e-02,  5.3657e-02,  ...,  4.8036e-03,
          -2.3912e-02,  1.8138e-01],
         [ 1.7729e-01, -6.5739e-02,  1.0575e-01,  ...,  8.3282e-02,
           2.1140e-01,  2.2936e-01],
         ...,
         [ 1.5048e-01,  6.5824e-02,  9.6436e-02,  ..., -9.6409e-03,
           2.2443e-02,  1.9736e-01],
         [ 2.8441e-01,  1.2827e-01,  1.6805e-01,  ..., -8.1877e-02,
           6.0534e-02,  8.5068e-02],
         [ 1.5346e-01,  6.1417e-02,  4.5446e-02,  ...,  4.2976e-02,
          -3.5593e-02,  2.0202e-01]],

        ...,

        [[ 1.4383e-01,  5.7049e-02,  3.9843e-02,  ..., -3.6265e-03,
          -4.0134e-02,  1.3164e-01],
         [-1.0873e-01, -4.5042e-01,  4.3080e-01,  ..., -7.9948e-01,
           6.6752e-01,  3.2727e-01],
         [ 1.3082e-01,  3.3356e-02,  9.4448e-02,  ..., -6.3894e-02,
           9.9523e-02,  1.0481e-01],
         ...,
         [ 2.0403e-01, -1.1052e-03,  1.1084e-01,  ..., -3.2049e-02,
          -1.0439e-02,  1.8488e-01],
         [ 1.7946e-01,  1.4091e-03,  1.0914e-01,  ..., -1.3732e-01,
          -1.9396e-02,  1.8728e-01],
         [ 1.6193e-01,  4.9411e-02,  3.4366e-02,  ...,  2.6171e-03,
          -4.5542e-03,  2.1330e-01]],

        [[ 1.7051e-01,  1.1486e-01,  6.3614e-02,  ...,  5.4797e-03,
          -2.0564e-02,  1.4196e-01],
         [ 2.3385e-01,  6.3729e-02,  1.3963e-01,  ...,  4.1603e-02,
          -2.7710e-02,  1.7561e-01],
         [ 2.3417e-01,  1.6674e-01,  2.2673e-01,  ..., -5.1315e-02,
           7.4699e-02,  2.2532e-01],
         ...,
         [ 4.4562e-01,  1.7486e-01,  2.0132e-01,  ..., -1.5164e-01,
           1.6836e-01,  2.2481e-01],
         [ 1.8967e-01,  6.9985e-02,  1.3985e-01,  ..., -9.6957e-02,
           4.4805e-02,  2.2303e-01],
         [ 4.2267e-01, -1.6500e-01, -7.7410e-02,  ..., -7.7896e-01,
          -2.0243e-01, -6.4543e-01]],

        [[ 1.4769e-01,  9.6552e-02,  3.1767e-02,  ...,  4.2564e-03,
          -2.1373e-02,  1.2421e-01],
         [ 3.1786e-01,  2.7704e-02,  2.2839e-01,  ..., -2.0022e-02,
           3.3020e-01,  4.0324e-01],
         [ 1.5523e-01,  2.2117e-02,  1.0030e-01,  ...,  1.4113e-02,
          -2.2007e-02,  1.4724e-01],
         ...,
         [ 2.1219e-01,  6.2299e-02,  7.4447e-02,  ...,  1.0972e-02,
           2.0092e-02,  1.9456e-01],
         [ 2.1944e-01,  3.3767e-03,  4.9296e-02,  ..., -2.5672e-02,
          -8.3384e-03,  2.4003e-01],
         [ 1.5800e-01,  5.5057e-02,  9.0356e-02,  ..., -4.1321e-02,
           3.6564e-02,  2.0163e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
This is my image_atts: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0') 

This is my labels: tensor([[-100, -100, -100, -100, -100, -100, -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100],
        [-100, -100, 2158, -100, -100, -100, -100, -100],
        [-100, -100, -100, -100, -100, -100, -100, -100],
        [-100, -100, -100, 2038, -100, 2304, -100, -100],
        [-100, 2023, -100, -100, -100, -100, -100, -100],
        [-100, -100, -100, 2003, -100, -100, -100, -100],
        [-100, -100, -100, -100, -100, 6379, -100, -100],
        [-100, -100, -100, -100, -100, 3239, -100, 2229]], device='cuda:0')
This is my prediction: tensor([[[5.6226e-12, 1.6026e-07, 3.2478e-08,  ..., 4.1198e-09,
          1.0895e-07, 2.3464e-08],
         [2.2919e-13, 1.1743e-10, 9.8578e-11,  ..., 1.7544e-10,
          2.1540e-09, 2.5436e-10],
         [5.6810e-13, 9.1185e-11, 6.6084e-11,  ..., 2.4179e-11,
          1.6985e-11, 1.3440e-12],
         ...,
         [1.2136e-11, 1.4673e-10, 3.4816e-11,  ..., 4.4236e-11,
          2.4536e-10, 1.9785e-10],
         [2.7483e-10, 6.0874e-10, 1.0665e-09,  ..., 7.5130e-10,
          2.0157e-11, 6.7137e-10],
         [7.6234e-10, 2.1511e-09, 6.2635e-09,  ..., 2.4381e-09,
          8.7509e-11, 2.4766e-09]],

        [[1.0409e-11, 2.5415e-07, 5.1092e-08,  ..., 6.9910e-09,
          9.6246e-08, 1.4929e-08],
         [1.5589e-12, 2.0618e-10, 2.2532e-10,  ..., 2.2607e-10,
          2.3073e-09, 4.4346e-10],
         [3.0628e-13, 3.0327e-10, 6.3566e-11,  ..., 3.6483e-11,
          4.2116e-11, 6.1250e-12],
         ...,
         [1.0304e-10, 4.5353e-10, 8.1926e-11,  ..., 6.2414e-11,
          2.8603e-10, 4.6862e-10],
         [2.7949e-09, 6.7667e-06, 6.9192e-07,  ..., 6.4851e-08,
          2.1475e-06, 1.5007e-07],
         [1.7532e-11, 5.5643e-07, 1.8534e-07,  ..., 1.9586e-08,
          4.6062e-07, 6.7830e-08]],

        [[7.8155e-12, 6.8313e-07, 8.4591e-08,  ..., 2.9844e-08,
          2.3344e-07, 9.8004e-08],
         [1.0012e-12, 2.1349e-10, 2.9531e-10,  ..., 1.6905e-10,
          2.2939e-09, 6.8643e-10],
         [4.4474e-13, 5.3270e-11, 3.3866e-10,  ..., 4.9880e-12,
          1.6317e-10, 2.5511e-11],
         ...,
         [1.3343e-10, 1.7408e-09, 2.1060e-10,  ..., 4.5254e-10,
          3.8097e-11, 4.4283e-10],
         [2.3997e-09, 3.4884e-06, 7.5701e-07,  ..., 1.8892e-08,
          1.3785e-06, 1.9689e-07],
         [7.9561e-12, 2.8802e-07, 2.1609e-08,  ..., 2.4107e-08,
          1.1414e-07, 7.5920e-09]],

        ...,

        [[1.2539e-10, 7.9415e-08, 2.3368e-07,  ..., 1.9018e-08,
          4.9418e-08, 4.9263e-08],
         [1.2973e-11, 1.4678e-10, 5.8721e-10,  ..., 2.8976e-10,
          3.9886e-10, 2.2284e-10],
         [1.2100e-13, 1.7051e-10, 1.6052e-10,  ..., 6.4645e-11,
          7.1720e-10, 1.1579e-10],
         ...,
         [1.5720e-11, 4.9638e-11, 3.9968e-11,  ..., 2.6579e-11,
          3.1193e-10, 1.2088e-10],
         [5.1980e-10, 3.4914e-10, 6.4918e-10,  ..., 4.2741e-10,
          2.5299e-11, 1.4160e-10],
         [7.7056e-10, 6.3238e-10, 1.1272e-09,  ..., 1.3086e-09,
          1.0768e-10, 2.8203e-10]],

        [[9.6099e-11, 7.8858e-07, 6.7840e-07,  ..., 3.7887e-08,
          1.8840e-07, 3.5700e-08],
         [1.0499e-12, 6.0259e-11, 3.7725e-11,  ..., 1.5317e-12,
          1.4859e-10, 2.5944e-11],
         [2.1845e-13, 4.3781e-10, 4.2447e-10,  ..., 6.9494e-11,
          8.1525e-12, 1.5843e-11],
         ...,
         [1.3345e-11, 8.5653e-10, 1.0659e-09,  ..., 7.0433e-10,
          5.1602e-12, 1.6731e-11],
         [7.1722e-12, 1.4276e-09, 5.1198e-10,  ..., 6.9288e-10,
          4.8301e-12, 1.7311e-11],
         [1.9331e-11, 1.3519e-09, 8.3255e-10,  ..., 4.0257e-10,
          1.1344e-11, 2.3785e-11]],

        [[1.7044e-10, 4.1503e-08, 1.3377e-07,  ..., 5.7176e-09,
          1.8079e-06, 3.1929e-08],
         [6.3413e-13, 4.7251e-11, 5.6246e-11,  ..., 1.8424e-11,
          1.4437e-10, 3.7601e-11],
         [2.4843e-13, 4.5553e-11, 5.0216e-11,  ..., 5.7609e-12,
          5.0261e-11, 1.9902e-11],
         ...,
         [9.8574e-11, 1.4404e-09, 5.6708e-09,  ..., 2.5682e-09,
          5.8429e-09, 1.2454e-08],
         [1.9265e-09, 1.7663e-09, 2.3922e-09,  ..., 2.2041e-09,
          8.3143e-08, 5.2974e-09],
         [2.1136e-10, 1.4114e-10, 8.0572e-11,  ..., 9.8609e-11,
          1.3454e-10, 5.0061e-11]]], device='cuda:0')
This is my alpha: 0.0002935995302407516
This is my mlm_output: MaskedLMOutput(loss=tensor(1.9004, device='cuda:0', grad_fn=<AddBackward0>), logits=tensor([[[-26.1673, -16.3332, -17.4221,  ..., -19.3560, -16.8200, -18.2709],
         [-20.9237, -14.3162, -13.9621,  ..., -14.4302, -12.2276, -13.4404],
         [-20.1239, -14.7750, -14.3859,  ..., -16.7994, -15.1163, -17.6509],
         ...,
         [-16.7680, -14.8951, -15.4985,  ..., -17.4272, -13.8230, -13.9900],
         [-10.6318, -10.4140,  -9.3026,  ...,  -8.9186, -11.9693,  -9.8684],
         [-25.3275, -15.9572, -16.4141,  ..., -18.3555, -14.7294, -16.9521]],

        [[-21.6656, -16.8204, -18.1557,  ..., -21.2703, -16.7858, -19.4321],
         [-20.3143, -15.6962, -15.7424,  ..., -15.3246, -12.3142, -13.7507],
         [-21.5261, -14.4518, -14.7610,  ..., -17.1346, -16.0138, -17.7662],
         ...,
         [-18.8083, -14.5101, -16.3719,  ..., -16.1109, -13.1954, -14.0117],
         [-12.2403, -10.6600,  -9.3704,  ...,  -9.2633, -11.6299, -11.5084],
         [-13.1598, -10.1198,  -9.5922,  ...,  -9.5071, -11.5628, -11.6133]],

        [[-26.0633, -14.5706, -16.6168,  ..., -19.2920, -14.5024, -15.6515],
         [-19.3284, -13.5762, -13.8121,  ..., -14.4926, -10.9700, -11.7273],
         [-20.1521, -14.9063, -13.2215,  ..., -17.8103, -13.2084, -13.6204],
         ...,
         [-14.2684, -13.6765, -13.0919,  ..., -13.0705, -14.1121, -11.7202],
         [-25.1853, -14.3007, -15.3469,  ..., -18.1034, -14.4486, -16.1055],
         [-10.8624,  -9.4720, -10.0111,  ..., -10.1869, -12.3075, -10.3400]],

        ...,

        [[-25.1141, -18.9883, -18.1642,  ..., -21.2565, -17.4835, -21.0115],
         [-18.4311, -17.1106, -14.7104,  ..., -16.5812, -15.3390, -14.8835],
         [-23.6262, -14.9346, -15.3868,  ..., -16.9535, -13.6797, -15.8394],
         ...,
         [-16.6692, -16.5970, -16.1377,  ..., -15.7024, -13.9890, -14.5407],
         [-10.5619, -11.1887, -10.1297,  ..., -10.8610, -12.8977, -11.9899],
         [ -9.6211, -11.2431, -10.2629,  ..., -10.5360, -12.7048, -12.3279]],

        [[-25.9070, -15.5251, -16.2128,  ..., -19.4814, -15.4794, -16.0620],
         [-23.0490, -16.8681, -18.1329,  ..., -19.3838, -14.1550, -16.1042],
         [-21.9720, -14.0651, -13.5556,  ..., -14.9907, -15.7606, -15.5245],
         ...,
         [-12.8301,  -9.7949,  -9.3522,  ...,  -8.8506, -12.1869, -11.5863],
         [-13.1451, -10.0017, -10.2098,  ...,  -9.5770, -12.3338, -11.6338],
         [-12.8776,  -9.5350,  -8.9441,  ...,  -9.1539, -11.8358, -11.0125]],

        [[-24.9737, -19.1755, -18.4713,  ..., -20.6228, -15.1489, -19.3756],
         [-23.2183, -17.1531, -15.3044,  ..., -18.0688, -14.5765, -15.6702],
         [-21.3140, -15.4276, -15.6206,  ..., -17.7378, -14.3985, -17.0800],
         ...,
         [-13.3938, -10.7807, -10.2818,  ..., -10.9723, -10.3980, -10.0745],
         [-13.2841, -13.9886, -13.1125,  ..., -13.2658,  -9.2197, -12.1348],
         [-11.0311, -11.3117, -11.7773,  ..., -11.3091, -11.3865, -11.8826]]],
       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None) and loss_mlm is mlm_output.loss
This is my mrtd_labels: tensor([[-100,    0,    0,    0,    0,    0, -100, -100],
        [-100,    0,    0,    0,    0,    0, -100, -100],
        [-100,    1,    1,    0,    1,    1, -100, -100],
        [-100,    0,    0,    1,    0,    0, -100, -100],
        [-100,    0,    0,    1,    1,    0, -100, -100],
        [-100,    0,    1,    1,    0,    0, -100, -100],
        [-100,    0,    0,    0,    0,    0, -100, -100],
        [-100,    1,    0,    0,    0,    0, -100, -100],
        [-100,    0,    0,    0,    0,    1, -100, -100],
        [-100,    1,    0,    0,    0,    0,    0,    0]], device='cuda:0')


#! find image???? ---------------------------------------------------------------
          [ 3.8290e-01,  6.8947e-01,  7.6246e-01,  ...,  1.3172e+00,
            1.3172e+00,  1.3172e+00]],

         [[-8.0661e-01, -7.7659e-01, -5.0645e-01,  ...,  6.6415e-01,
            4.8406e-01,  3.9401e-01],
          [-3.7138e-01, -3.4137e-01, -3.8639e-01,  ...,  7.6921e-01,
            6.0412e-01,  3.7901e-01],
          [-3.2636e-01, -1.9129e-01, -2.6633e-01,  ...,  7.2418e-01,
            6.9417e-01,  5.5910e-01],
          ...,
          [-2.0630e-01,  2.7395e-01,  7.5420e-01,  ...,  9.3429e-01,
            9.7932e-01,  1.0393e+00],
          [ 2.5894e-01,  5.8911e-01,  8.4425e-01,  ...,  1.3395e+00,
            1.3545e+00,  1.3545e+00],
          [ 3.4899e-01,  6.6415e-01,  7.5420e-01,  ...,  1.3245e+00,
            1.3245e+00,  1.3245e+00]],

         [[-3.5683e-01, -2.9995e-01, -1.5553e-02,  ...,  1.3496e+00,
            1.3638e+00,  1.3780e+00],
          [-2.4307e-01, -1.7197e-01, -1.2931e-01,  ...,  1.4491e+00,
            1.4776e+00,  1.3638e+00],
          [-3.9949e-01, -2.2885e-01, -2.0041e-01,  ...,  1.4065e+00,
            1.5629e+00,  1.5344e+00],
          ...,
          [-1.0087e-01,  3.5417e-01,  7.9499e-01,  ...,  9.5141e-01,
            9.9407e-01,  1.0510e+00],
          [ 3.3995e-01,  6.5279e-01,  8.9453e-01,  ...,  1.3354e+00,
            1.3354e+00,  1.3354e+00],
          [ 4.5371e-01,  7.5233e-01,  8.2343e-01,  ...,  1.3069e+00,
            1.3069e+00,  1.3069e+00]]],


        [[[-5.5140e-01,  3.8290e-01,  7.4786e-01,  ...,  8.2086e-01,
            7.9166e-01,  3.6830e-01],
          [-2.7403e-01,  5.7268e-01,  6.4567e-01,  ...,  6.1648e-01,
            4.2670e-01,  4.7139e-02],
          [ 9.0935e-02,  6.7487e-01,  7.0407e-01,  ...,  6.6027e-01,
            2.5152e-01, -1.4264e-01],
          ...,
          [-5.0760e-01,  1.7853e-01,  3.8290e-01,  ..., -9.8935e-01,
           -4.0451e-02, -3.3242e-01],
          [-3.4702e-01, -2.1563e-01, -2.0103e-01,  ..., -3.4702e-01,
            1.6393e-01, -6.6818e-01],
          [ 2.3692e-01,  3.2451e-01,  3.0991e-01,  ...,  3.3439e-03,
            7.6336e-02, -8.8716e-01]],

         [[-8.3662e-01,  7.8851e-02,  4.0902e-01,  ...,  3.7901e-01,
            3.4899e-01, -8.6235e-02],
          [-5.5148e-01,  2.7395e-01,  3.0397e-01,  ...,  1.9891e-01,
            3.8118e-03, -4.0140e-01],
          [-1.9129e-01,  3.7901e-01,  3.7901e-01,  ...,  2.4394e-01,
           -1.7628e-01, -5.8149e-01],
          ...,
          [-5.5148e-01,  1.5389e-01,  3.3398e-01,  ..., -9.4168e-01,
            1.8820e-02, -2.8134e-01],
          [-3.1135e-01, -1.9129e-01, -2.2130e-01,  ..., -2.8134e-01,
            2.4394e-01, -6.2651e-01],
          [ 3.3398e-01,  4.0902e-01,  3.3398e-01,  ...,  7.8851e-02,
            1.3888e-01, -8.5163e-01]],

         [[-6.4124e-01,  2.4041e-01,  5.2481e-01,  ...,  3.6839e-01,
            3.1151e-01, -1.1509e-01],
          [-3.7105e-01,  4.1105e-01,  4.2527e-01,  ...,  2.2619e-01,
            4.1327e-02, -3.5683e-01],
          [-1.5553e-02,  5.1059e-01,  4.9637e-01,  ...,  3.6839e-01,
           -5.8213e-02, -4.5637e-01],
          ...,
          [-3.4261e-01,  3.1151e-01,  4.6793e-01,  ..., -5.9858e-01,
            3.3995e-01,  6.9767e-02],
          [-1.5775e-01, -5.8213e-02, -7.2433e-02,  ...,  2.7107e-02,
            5.3903e-01, -2.5729e-01],
          [ 4.3949e-01,  4.9637e-01,  4.3949e-01,  ...,  3.5417e-01,
            4.5371e-01, -4.5637e-01]]],


        [[[ 3.8290e-01,  1.7853e-01,  1.4933e-01,  ..., -1.1344e-01,
           -8.4247e-02, -4.0451e-02],
          [-7.5577e-01, -5.3680e-01, -2.7403e-01,  ..., -7.5577e-01,
           -7.1198e-01, -7.5577e-01],
          [-8.5796e-01, -7.7037e-01, -8.8716e-01,  ..., -1.0477e+00,
           -9.4555e-01, -9.3096e-01],
          ...,
          [ 6.4567e-01,  6.1648e-01,  6.0188e-01,  ...,  2.0772e-01,
            1.9312e-01,  2.2232e-01],
          [ 6.1648e-01,  6.1648e-01,  5.7268e-01,  ...,  3.8290e-01,
            3.9750e-01,  4.2670e-01],
          [ 3.3911e-01,  3.5371e-01,  3.6830e-01,  ...,  2.8071e-01,
            2.3692e-01,  2.3692e-01]],

         [[ 6.7916e-01,  4.5404e-01,  4.3904e-01,  ...,  1.2387e-01,
            1.0887e-01,  1.3888e-01],
          [-5.9650e-01, -3.7138e-01, -1.1625e-01,  ..., -5.6648e-01,
           -5.6648e-01, -6.4152e-01],
          [-8.0661e-01, -7.1656e-01, -8.5163e-01,  ..., -9.4168e-01,
           -8.5163e-01, -8.3662e-01],
          ...,
          [ 6.7916e-01,  6.4915e-01,  6.3414e-01,  ...,  3.0397e-01,
            2.8896e-01,  3.1897e-01],
          [ 6.9417e-01,  6.7916e-01,  6.4915e-01,  ...,  4.8406e-01,
            4.9907e-01,  5.2908e-01],
          [ 4.2403e-01,  4.3904e-01,  4.5404e-01,  ...,  3.7901e-01,
            3.3398e-01,  3.3398e-01]],

         [[ 3.5417e-01,  1.5509e-01,  1.4087e-01,  ..., -2.1463e-01,
           -2.1463e-01, -1.8619e-01],
          [-6.6968e-01, -4.5637e-01, -2.0041e-01,  ..., -7.2656e-01,
           -7.1234e-01, -7.5500e-01],
          [-6.6968e-01, -5.8436e-01, -7.1234e-01,  ..., -8.5454e-01,
           -7.5500e-01, -7.4078e-01],
          ...,
          [ 7.9499e-01,  7.6655e-01,  7.3811e-01,  ...,  4.3949e-01,
            4.2527e-01,  4.5371e-01],
          [ 8.2343e-01,  8.0921e-01,  7.8077e-01,  ...,  6.1013e-01,
            6.2435e-01,  6.5279e-01],
          [ 5.5325e-01,  5.6747e-01,  5.8169e-01,  ...,  5.1059e-01,
            4.6793e-01,  4.6793e-01]]],


        ...,


        [[[-9.4555e-01, -9.4555e-01, -9.4555e-01,  ..., -9.8935e-01,
           -9.7475e-01, -9.8935e-01],
          [-9.4555e-01, -9.4555e-01, -9.4555e-01,  ..., -1.0623e+00,
           -9.8935e-01, -9.8935e-01],
          [-9.4555e-01, -9.4555e-01, -9.4555e-01,  ..., -1.1645e+00,
           -1.0623e+00, -1.0039e+00],
          ...,
          [ 4.2670e-01,  3.5371e-01,  3.5371e-01,  ..., -1.4264e-01,
           -2.8862e-01, -3.0322e-01],
          [ 3.8290e-01,  4.1210e-01,  4.1210e-01,  ...,  1.7942e-02,
            3.2541e-02,  1.4933e-01],
          [ 4.5590e-01,  4.9969e-01,  4.8509e-01,  ...,  1.4933e-01,
            2.9531e-01,  3.8290e-01]],

         [[-9.4168e-01, -9.4168e-01, -9.4168e-01,  ..., -9.4168e-01,
           -9.4168e-01, -9.4168e-01],
          [-9.4168e-01, -9.4168e-01, -9.4168e-01,  ..., -1.0017e+00,
           -9.2667e-01, -9.4168e-01],
          [-9.4168e-01, -9.4168e-01, -9.4168e-01,  ..., -1.0767e+00,
           -1.0017e+00, -9.5669e-01],
          ...,
          [ 5.2908e-01,  4.5404e-01,  4.5404e-01,  ..., -5.6219e-02,
           -2.0630e-01, -2.2130e-01],
          [ 4.8406e-01,  5.1408e-01,  5.1408e-01,  ...,  1.0887e-01,
            1.2387e-01,  2.4394e-01],
          [ 5.5910e-01,  6.0412e-01,  5.8911e-01,  ...,  2.4394e-01,
            3.9401e-01,  4.8406e-01]],

         [[-7.8344e-01, -7.8344e-01, -7.8344e-01,  ..., -7.6922e-01,
           -7.6922e-01, -7.9766e-01],
          [-7.8344e-01, -7.8344e-01, -7.8344e-01,  ..., -8.2610e-01,
           -7.6922e-01, -7.9766e-01],
          [-7.8344e-01, -7.8344e-01, -7.8344e-01,  ..., -9.3986e-01,
           -8.5454e-01, -8.2610e-01],
          ...,
          [ 6.8123e-01,  6.1013e-01,  6.1013e-01,  ...,  1.2665e-01,
           -1.5553e-02, -2.9773e-02],
          [ 6.3857e-01,  6.6701e-01,  6.6701e-01,  ...,  2.8307e-01,
            2.9729e-01,  4.1105e-01],
          [ 7.0967e-01,  7.5233e-01,  7.3811e-01,  ...,  4.1105e-01,
            5.5325e-01,  6.3857e-01]]],


        [[[-2.3023e-01, -6.9648e-02,  4.7139e-02,  ...,  3.5371e-01,
           -6.9648e-02, -2.1563e-01],
          [-1.2804e-01, -6.9648e-02,  1.7942e-02,  ..., -1.2804e-01,
           -6.9648e-02, -5.5050e-02],
          [ 1.5508e+00,  1.6822e+00,  1.5800e+00,  ..., -6.2439e-01,
           -5.8059e-01, -5.8059e-01],
          ...,
          [-8.4247e-02, -6.9648e-02,  4.7139e-02,  ...,  7.0407e-01,
            7.1867e-01,  7.6246e-01],
          [ 1.2013e-01,  1.6393e-01,  2.0772e-01,  ...,  7.1867e-01,
            7.3327e-01,  8.6465e-01],
          [ 2.3692e-01,  2.3692e-01,  2.8071e-01,  ...,  7.3327e-01,
            7.9166e-01,  8.7925e-01]],

         [[-5.6219e-02,  9.3858e-02,  1.8391e-01,  ...,  4.6905e-01,
            1.8820e-02, -1.6127e-01],
          [ 3.3398e-01,  3.6400e-01,  4.0902e-01,  ..., -1.1196e-02,
            4.8835e-02,  6.3843e-02],
          [ 1.9098e+00,  2.0149e+00,  1.9848e+00,  ..., -5.5148e-01,
           -5.2146e-01, -5.6648e-01],
          ...,
          [-1.9129e-01, -1.7628e-01, -5.6219e-02,  ...,  5.5910e-01,
            5.7411e-01,  6.1913e-01],
          [ 1.8820e-02,  6.3843e-02,  1.2387e-01,  ...,  5.7411e-01,
            5.8911e-01,  7.2418e-01],
          [ 9.3858e-02,  1.0887e-01,  1.3888e-01,  ...,  5.8911e-01,
            6.4915e-01,  7.3919e-01]],

         [[ 1.5509e-01, -2.9773e-02, -1.3329e-03,  ...,  4.5371e-01,
            2.7107e-02, -1.2931e-01],
          [ 1.0510e+00,  1.0083e+00,  1.0367e+00,  ..., -8.6653e-02,
           -5.8213e-02, -1.5553e-02],
          [ 1.9895e+00,  2.1032e+00,  2.0606e+00,  ..., -5.1326e-01,
           -5.4170e-01, -5.4170e-01],
          ...,
          [-2.4307e-01, -2.2885e-01, -1.1509e-01,  ...,  4.5371e-01,
            4.6793e-01,  5.1059e-01],
          [-8.6653e-02, -4.3993e-02,  1.2887e-02,  ...,  4.6793e-01,
            4.8215e-01,  6.1013e-01],
          [-2.9773e-02, -2.9773e-02,  1.2887e-02,  ...,  4.5371e-01,
            5.2481e-01,  6.1013e-01]]],


        [[[-6.9648e-02, -2.5853e-02, -6.9648e-02,  ...,  3.9750e-01,
            3.9750e-01,  3.9750e-01],
          [-9.8845e-02, -6.9648e-02, -1.2804e-01,  ...,  4.7049e-01,
            4.7049e-01,  4.7049e-01],
          [-5.5050e-02, -6.9648e-02, -1.5724e-01,  ...,  3.3911e-01,
            3.2451e-01,  3.0991e-01],
          ...,
          [ 1.7260e+00,  1.7260e+00,  1.7260e+00,  ...,  1.8281e+00,
            1.8281e+00,  1.8427e+00],
          [ 1.5216e+00,  1.5362e+00,  1.5654e+00,  ...,  1.8719e+00,
            1.8719e+00,  1.8719e+00],
          [ 1.1858e+00,  1.2004e+00,  1.2442e+00,  ...,  1.9157e+00,
            1.9157e+00,  1.9011e+00]],

         [[-1.1625e-01, -7.1227e-02, -1.1625e-01,  ...,  4.2403e-01,
            4.2403e-01,  4.2403e-01],
          [-1.4627e-01, -1.1625e-01, -1.7628e-01,  ...,  4.9907e-01,
            4.9907e-01,  4.9907e-01],
          [-1.1625e-01, -1.3126e-01, -2.2130e-01,  ...,  3.6400e-01,
            3.4899e-01,  3.3398e-01],
          ...,
          [ 1.8498e+00,  1.8498e+00,  1.8498e+00,  ...,  1.9698e+00,
            1.9698e+00,  1.9848e+00],
          [ 1.6247e+00,  1.6397e+00,  1.6697e+00,  ...,  2.0149e+00,
            2.0149e+00,  2.0149e+00],
          [ 1.2645e+00,  1.2795e+00,  1.3245e+00,  ...,  2.0599e+00,
            2.0599e+00,  2.0449e+00]],

         [[ 8.3987e-02,  1.2665e-01,  8.3987e-02,  ...,  6.8123e-01,
            6.6701e-01,  6.6701e-01],
          [ 5.5547e-02,  8.3987e-02,  2.7107e-02,  ...,  7.3811e-01,
            7.2389e-01,  7.0967e-01],
          [ 8.3987e-02,  5.5547e-02, -1.3329e-03,  ...,  5.8169e-01,
            5.5325e-01,  5.5325e-01],
          ...,
          [ 2.0321e+00,  2.0179e+00,  2.0037e+00,  ...,  2.0464e+00,
            2.0464e+00,  2.0606e+00],
          [ 1.8899e+00,  1.8899e+00,  1.9184e+00,  ...,  2.0890e+00,
            2.0890e+00,  2.0890e+00],
          [ 1.6055e+00,  1.6198e+00,  1.6624e+00,  ...,  2.1317e+00,
            2.1317e+00,  2.1175e+00]]]], device='cuda:0')
This is my image_embeds: tensor([[[ 1.6812e-01,  7.7459e-02,  4.2383e-02,  ...,  2.7137e-02,
          -3.3812e-02,  1.1747e-01],
         [ 1.7945e-01,  1.1131e-01,  1.7084e-01,  ..., -6.0692e-02,
           3.1632e-01,  3.5516e-01],
         [ 2.3307e-01, -1.7064e-02,  1.4258e-01,  ..., -2.4142e-02,
           2.7814e-02,  1.5903e-01],
         ...,
         [ 1.6892e-01,  1.4093e-01,  1.7248e-01,  ..., -2.5796e-02,
           2.6723e-02,  2.1252e-01],
         [ 2.4448e-01,  5.5012e-02,  1.1823e-01,  ...,  2.4236e-02,
          -5.8076e-03,  1.4984e-01],
         [ 2.0357e-01,  6.9421e-02,  9.1794e-02,  ...,  2.4135e-02,
          -1.5578e-02,  1.5615e-01]],

        [[ 1.5092e-01,  9.2454e-02,  3.2357e-02,  ...,  4.3368e-03,
          -3.1892e-02,  1.3314e-01],
         [ 1.3152e-01, -1.4082e-01, -4.6497e-02,  ..., -2.1709e-01,
           1.3172e+00, -4.6869e-01],
         [ 1.9281e-01,  2.6161e-02,  1.0753e-01,  ...,  1.8374e-03,
           3.1788e-02,  1.6044e-01],
         ...,
         [ 1.4720e-01,  6.9640e-02,  7.2628e-02,  ...,  9.1778e-04,
          -1.3090e-02,  2.0855e-01],
         [ 2.0165e-01,  7.8447e-02,  8.2403e-02,  ..., -1.6394e-02,
           1.4769e-02,  1.8123e-01],
         [ 1.5272e-01,  2.2240e-02,  1.0875e-01,  ..., -4.2184e-03,
          -1.5018e-02,  1.3871e-01]],

        [[ 1.1367e-01,  9.7785e-02,  1.7522e-02,  ...,  1.2276e-02,
          -3.2256e-02,  1.4261e-01],
         [ 1.6653e-01,  1.0670e-01,  7.7512e-02,  ..., -8.1035e-02,
           7.5472e-02,  2.0185e-01],
         [-5.8466e-01, -1.8388e-01, -1.5106e-01,  ...,  2.0084e-01,
           7.7185e-01,  9.3203e-02],
         ...,
         [ 2.2931e-01,  7.2640e-02,  3.7813e-03,  ..., -1.1247e-01,
           4.7151e-02,  2.5653e-01],
         [ 1.6135e-01,  8.1744e-02,  1.0226e-01,  ..., -6.1256e-02,
           1.0100e-01,  1.8936e-01],
         [ 5.3249e-01, -4.9593e-01, -2.0276e-01,  ..., -1.4938e-01,
           6.7260e-01, -1.4676e-02]],

        ...,

        [[ 1.5508e-01,  9.8143e-02,  3.9238e-02,  ..., -1.3610e-02,
          -2.0065e-02,  1.2937e-01],
         [ 1.9160e-01,  4.0594e-02,  2.7191e-02,  ...,  3.0124e-02,
          -2.7034e-03,  2.1502e-01],
         [ 2.1747e-01,  5.7696e-02,  9.6475e-02,  ...,  1.3509e-02,
           3.0390e-02,  1.5720e-01],
         ...,
         [ 1.6883e-01,  6.0633e-02,  9.2827e-02,  ...,  8.7872e-04,
           1.1276e-02,  1.5155e-01],
         [ 3.5077e-01,  7.3602e-02,  1.5864e-01,  ..., -1.5367e-03,
           1.2103e-01,  1.4023e-02],
         [ 1.8802e-01,  7.1971e-02,  7.6704e-02,  ..., -2.0532e-02,
           4.0548e-03,  1.1993e-01]],

        [[ 1.1738e-01,  9.3652e-02,  1.9649e-02,  ...,  2.7031e-02,
          -2.5312e-02,  1.6107e-01],
         [ 1.2900e-01,  1.3290e-01,  1.2464e-01,  ..., -2.4215e-02,
           3.5202e-02,  1.4839e-01],
         [-9.7341e-01, -7.3717e-01, -2.4112e-01,  ...,  9.7872e-03,
           1.2036e+00, -2.0142e-01],
         ...,
         [-1.0188e-01, -1.3913e-03, -2.0646e-03,  ..., -2.4479e-01,
           1.1105e-01,  3.4958e-01],
         [ 1.0972e-01,  7.4368e-02,  6.0495e-02,  ..., -1.3778e-01,
           1.5279e-01,  2.6887e-01],
         [ 1.3702e-01,  5.5880e-02,  8.6597e-02,  ...,  3.0532e-02,
          -2.7090e-02,  1.9995e-01]],

        [[ 1.5121e-01,  9.9012e-02,  5.0330e-02,  ..., -1.5221e-02,
          -1.9118e-02,  1.5139e-01],
         [ 1.4063e-01,  7.1586e-02,  1.6257e-01,  ...,  1.1827e-03,
           9.3583e-02,  1.7900e-01],
         [ 1.4929e-01,  4.7819e-02,  8.7260e-02,  ..., -6.9242e-03,
           5.6560e-02,  1.9101e-01],
         ...,
         [ 1.7614e-01,  6.3768e-02,  3.6941e-02,  ...,  5.1652e-03,
          -6.3345e-03,  2.4096e-01],
         [ 1.7528e-01,  4.9959e-02,  1.5250e-01,  ..., -1.0962e-02,
           2.1870e-02,  2.1696e-01],
         [ 1.8597e-01,  1.3460e-01,  1.4943e-01,  ...,  5.0404e-03,
          -3.7696e-03,  1.7378e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
This is my labels = input_ids.clone(): tensor([[ 101, 1996, 2158, 2003, 3061, 2007],
        [ 101, 1996, 2450, 2038, 2146, 2601],
        [ 101, 1996, 2611, 2003, 4147, 2014],
        [ 101, 2023, 2450, 2003, 4147, 1037],
        [ 101, 1037, 2158, 4147, 1037, 2317],
        [ 101, 1037, 2711, 2003, 3788, 2185],
        [ 101, 1996, 2158, 2003, 4147, 1037],
        [ 101, 2023, 2711, 2038, 2460, 2601],
        [ 101, 1996, 2711, 2003, 4147, 1037],
        [ 101, 1996, 2450, 2038, 2601, 2606]], device='cuda:0')
This is my  self.text_encoder.config.vocab_size: 30522
This is input_ids: tensor([[ 101, 1996, 2158, 2003, 3061, 2007],
        [ 101, 1996, 2450, 2038, 2146,  103],
        [ 101, 1996,  103, 2003, 4147, 2014],
        [ 101, 2023, 2450, 2003, 4147, 1037],
        [ 101, 1037, 2158,  103, 1037, 2317],
        [ 101, 1037, 2711, 2003, 3788, 2185],
        [ 101,  103, 2158, 2003, 4147,  103],
        [ 101, 2023, 2711, 2038, 2460, 2601],
        [ 101, 1996, 2711, 2003, 4147, 1037],
        [ 101, 1996, 2450, 2038, 2601,  103]], device='cuda:0')


This is targets=labels (format=(labels)): tensor([[-100, -100, -100, -100, -100, -100],
        [-100, -100, -100, -100, -100, 2601],
        [-100, -100, 2611, -100, -100, -100],
        [-100, -100, -100, -100, -100, -100],
        [-100, -100, -100, 4147, -100, -100],
        [-100, -100, -100, -100, -100, -100],
        [-100, 1996, -100, -100, -100, 1037],
        [-100, -100, -100, -100, -100, -100],
        [-100, -100, -100, -100, -100, -100],
        [-100, -100, -100, -100, -100, 2606]], device='cuda:0')
When it is mlm output: 


This is mlm_output: MaskedLMOutput(loss=tensor(4.0244, device='cuda:0', grad_fn=<AddBackward0>), logits=tensor([[[-25.9538, -16.0904, -16.9402,  ..., -20.0059, -16.2852, -18.6348],
         [-20.8929, -15.4547, -14.1227,  ..., -14.8561, -13.0428, -15.1166],
         [-22.6841, -15.3722, -15.5586,  ..., -19.0142, -17.3503, -18.9108],
         [-24.6156, -15.0719, -17.7361,  ..., -18.9399, -15.0547, -16.4838],
         [-20.9998, -14.1055, -15.2173,  ..., -15.7790, -12.9358, -15.2316],
         [-17.7717, -13.1694, -14.0167,  ..., -17.8858, -13.5094, -14.8307]],

        [[-24.7852, -15.2070, -15.8833,  ..., -19.2476, -14.7530, -15.4475],
         [-19.5097, -14.5979, -13.9652,  ..., -14.7831, -12.7153, -13.8897],
         [-21.4651, -14.0881, -13.3835,  ..., -15.8800, -16.2162, -15.5784],
         [-20.2426, -16.1186, -17.1906,  ..., -17.3859, -13.3340, -14.7952],
         [-19.3170, -14.5669, -16.0779,  ..., -14.9236, -13.8477, -13.8128],
         [-12.7050,  -8.9836, -10.0641,  ..., -10.5624, -13.7037,  -9.9741]],

        [[-25.9689, -17.1327, -17.0647,  ..., -18.6419, -15.2281, -16.1439],
         [-17.6277, -13.8911, -13.3593,  ..., -13.5913, -11.4119, -11.9835],
         [-13.7871, -10.9770, -10.9427,  ..., -12.1039, -11.8724, -10.6759],
         [-23.7548, -16.2113, -16.6897,  ..., -18.3564, -15.8008, -15.5037],
         [-18.5622, -13.2427, -16.2547,  ..., -15.4660, -14.7422, -15.0274],
         [-12.1613, -10.9294, -11.2316,  ...,  -8.4510, -11.7232,  -7.3238]],

        ...,

        [[-25.1408, -17.0682, -15.7599,  ..., -19.1841, -14.8261, -16.6648],
         [-21.6670, -16.1962, -14.3909,  ..., -17.4561, -14.9937, -14.3364],
         [-26.5599, -13.7296, -15.4611,  ..., -17.0845, -13.5405, -13.9601],
         [-21.5976, -18.6392, -18.6659,  ..., -19.6747, -15.1278, -17.1086],
         [-18.1612, -13.3724, -14.9234,  ..., -16.0499, -12.3192, -11.0569],
         [-16.3946, -12.8122, -13.8082,  ..., -13.9240, -15.4160, -13.4523]],

        [[-26.5355, -15.0627, -15.3652,  ..., -17.3603, -15.2801, -16.7013],
         [-21.9856, -15.1095, -14.4487,  ..., -14.1956, -11.4445, -13.0965],
         [-23.1037, -12.5646, -13.0188,  ..., -13.8827, -11.6131, -13.1196],
         [-22.4346, -15.7287, -16.5498,  ..., -16.9934, -13.5240, -13.8969],
         [-19.9892, -12.2944, -14.9870,  ..., -16.7104, -13.9161, -15.4113],
         [-16.0915, -13.9262, -14.0230,  ..., -15.1312, -14.1622, -12.3746]],

        [[-24.8931, -15.8718, -15.8277,  ..., -19.0522, -14.7856, -15.7431],
         [-22.1628, -15.7167, -14.8589,  ..., -16.3529, -12.7367, -14.0223],
         [-21.2303, -14.7240, -14.3300,  ..., -17.8667, -16.9476, -17.8136],
         [-21.0658, -16.6612, -17.2124,  ..., -18.8320, -12.6268, -16.2500],
         [-19.7244, -14.7084, -16.4168,  ..., -16.4294, -13.8852, -13.6535],
         [-11.7181, -10.4268, -10.0929,  ..., -11.8916, -13.9373, -10.7553]]],
       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None) 

This is my text1: {'input_ids': tensor([[ 101, 1996, 2158, 2003, 3061, 2007],
        [ 101, 1996, 2450, 2038, 2146, 2601],
        [ 101, 1996, 2611, 2003, 4147, 2014],
        [ 101, 2023, 2450, 2003, 4147, 1037],
        [ 101, 1037, 2158, 4147, 1037, 2317],
        [ 101, 1037, 2711, 2003, 3788, 2185],
        [ 101, 1996, 2158, 2003, 4147, 1037],
        [ 101, 2023, 2711, 2038, 2460, 2601],
        [ 101, 1996, 2711, 2003, 4147, 1037],
        [ 101, 1996, 2450, 2038, 2601, 2606]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1]], device='cuda:0')} 

This is my image_embeds: tensor([[[ 1.6812e-01,  7.7459e-02,  4.2383e-02,  ...,  2.7137e-02,
          -3.3812e-02,  1.1747e-01],
         [ 1.7945e-01,  1.1131e-01,  1.7084e-01,  ..., -6.0692e-02,
           3.1632e-01,  3.5516e-01],
         [ 2.3307e-01, -1.7064e-02,  1.4258e-01,  ..., -2.4142e-02,
           2.7814e-02,  1.5903e-01],
         ...,
         [ 1.6892e-01,  1.4093e-01,  1.7248e-01,  ..., -2.5796e-02,
           2.6723e-02,  2.1252e-01],
         [ 2.4448e-01,  5.5012e-02,  1.1823e-01,  ...,  2.4236e-02,
          -5.8076e-03,  1.4984e-01],
         [ 2.0357e-01,  6.9421e-02,  9.1794e-02,  ...,  2.4135e-02,
          -1.5578e-02,  1.5615e-01]],

        [[ 1.5092e-01,  9.2454e-02,  3.2357e-02,  ...,  4.3368e-03,
          -3.1892e-02,  1.3314e-01],
         [ 1.3152e-01, -1.4082e-01, -4.6497e-02,  ..., -2.1709e-01,
           1.3172e+00, -4.6869e-01],
         [ 1.9281e-01,  2.6161e-02,  1.0753e-01,  ...,  1.8374e-03,
           3.1788e-02,  1.6044e-01],
         ...,
         [ 1.4720e-01,  6.9640e-02,  7.2628e-02,  ...,  9.1778e-04,
          -1.3090e-02,  2.0855e-01],
         [ 2.0165e-01,  7.8447e-02,  8.2403e-02,  ..., -1.6394e-02,
           1.4769e-02,  1.8123e-01],
         [ 1.5272e-01,  2.2240e-02,  1.0875e-01,  ..., -4.2184e-03,
          -1.5018e-02,  1.3871e-01]],

        [[ 1.1367e-01,  9.7785e-02,  1.7522e-02,  ...,  1.2276e-02,
          -3.2256e-02,  1.4261e-01],
         [ 1.6653e-01,  1.0670e-01,  7.7512e-02,  ..., -8.1035e-02,
           7.5472e-02,  2.0185e-01],
         [-5.8466e-01, -1.8388e-01, -1.5106e-01,  ...,  2.0084e-01,
           7.7185e-01,  9.3203e-02],
         ...,
         [ 2.2931e-01,  7.2640e-02,  3.7813e-03,  ..., -1.1247e-01,
           4.7151e-02,  2.5653e-01],
         [ 1.6135e-01,  8.1744e-02,  1.0226e-01,  ..., -6.1256e-02,
           1.0100e-01,  1.8936e-01],
         [ 5.3249e-01, -4.9593e-01, -2.0276e-01,  ..., -1.4938e-01,
           6.7260e-01, -1.4676e-02]],

        ...,

        [[ 1.5508e-01,  9.8143e-02,  3.9238e-02,  ..., -1.3610e-02,
          -2.0065e-02,  1.2937e-01],
         [ 1.9160e-01,  4.0594e-02,  2.7191e-02,  ...,  3.0124e-02,
          -2.7034e-03,  2.1502e-01],
         [ 2.1747e-01,  5.7696e-02,  9.6475e-02,  ...,  1.3509e-02,
           3.0390e-02,  1.5720e-01],
         ...,
         [ 1.6883e-01,  6.0633e-02,  9.2827e-02,  ...,  8.7872e-04,
           1.1276e-02,  1.5155e-01],
         [ 3.5077e-01,  7.3602e-02,  1.5864e-01,  ..., -1.5367e-03,
           1.2103e-01,  1.4023e-02],
         [ 1.8802e-01,  7.1971e-02,  7.6704e-02,  ..., -2.0532e-02,
           4.0548e-03,  1.1993e-01]],

        [[ 1.1738e-01,  9.3652e-02,  1.9649e-02,  ...,  2.7031e-02,
          -2.5312e-02,  1.6107e-01],
         [ 1.2900e-01,  1.3290e-01,  1.2464e-01,  ..., -2.4215e-02,
           3.5202e-02,  1.4839e-01],
         [-9.7341e-01, -7.3717e-01, -2.4112e-01,  ...,  9.7872e-03,
           1.2036e+00, -2.0142e-01],
         ...,
         [-1.0188e-01, -1.3913e-03, -2.0646e-03,  ..., -2.4479e-01,
           1.1105e-01,  3.4958e-01],
         [ 1.0972e-01,  7.4368e-02,  6.0495e-02,  ..., -1.3778e-01,
           1.5279e-01,  2.6887e-01],
         [ 1.3702e-01,  5.5880e-02,  8.6597e-02,  ...,  3.0532e-02,
          -2.7090e-02,  1.9995e-01]],

        [[ 1.5121e-01,  9.9012e-02,  5.0330e-02,  ..., -1.5221e-02,
          -1.9118e-02,  1.5139e-01],
         [ 1.4063e-01,  7.1586e-02,  1.6257e-01,  ...,  1.1827e-03,
           9.3583e-02,  1.7900e-01],
         [ 1.4929e-01,  4.7819e-02,  8.7260e-02,  ..., -6.9242e-03,
           5.6560e-02,  1.9101e-01],
         ...,
         [ 1.7614e-01,  6.3768e-02,  3.6941e-02,  ...,  5.1652e-03,
          -6.3345e-03,  2.4096e-01],
         [ 1.7528e-01,  4.9959e-02,  1.5250e-01,  ..., -1.0962e-02,
           2.1870e-02,  2.1696e-01],
         [ 1.8597e-01,  1.3460e-01,  1.4943e-01,  ...,  5.0404e-03,
          -3.7696e-03,  1.7378e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
This is my labels: tensor([[-100, -100, -100, -100, -100, -100],
        [-100, -100, -100, -100, -100, 2601],
        [-100, -100, 2611, -100, -100, -100],
        [-100, -100, -100, -100, -100, -100],
        [-100, -100, -100, 4147, -100, -100],
        [-100, -100, -100, -100, -100, -100],
        [-100, 1996, -100, -100, -100, 1037],
        [-100, -100, -100, -100, -100, -100],
        [-100, -100, -100, -100, -100, -100],
        [-100, -100, -100, -100, -100, 2606]], device='cuda:0')
This is my prediction: tensor([[[7.7809e-12, 3.8917e-07, 1.2266e-07,  ..., 1.0116e-08,
          1.3289e-07, 5.2149e-08],
         [2.5003e-12, 4.4145e-10, 7.0672e-10,  ..., 2.8905e-10,
          2.2562e-09, 8.4251e-10],
         [9.7471e-14, 7.2561e-11, 1.4408e-10,  ..., 5.5274e-12,
          3.4781e-11, 3.9355e-12],
         [8.7624e-15, 1.2795e-10, 1.7764e-11,  ..., 1.5127e-11,
          1.7443e-10, 5.3008e-11],
         [6.2523e-13, 4.7536e-10, 4.0774e-11,  ..., 3.7890e-11,
          8.1427e-10, 1.0758e-10],
         [3.1738e-11, 3.1218e-10, 2.1055e-10,  ..., 7.2756e-12,
          2.6976e-10, 1.2798e-10]],

        [[6.2990e-12, 1.0384e-07, 2.8826e-08,  ..., 5.0400e-09,
          7.4977e-07, 2.3476e-07],
         [8.1873e-12, 3.9488e-10, 4.4297e-10,  ..., 3.2633e-10,
          3.5726e-09, 9.1943e-10],
         [4.9813e-14, 7.4451e-11, 7.1676e-11,  ..., 4.9961e-12,
          5.3135e-12, 3.2779e-12],
         [3.0689e-13, 2.3287e-11, 1.7724e-11,  ..., 5.3746e-12,
          3.2455e-10, 5.8391e-11],
         [2.3106e-12, 2.5614e-10, 4.5299e-11,  ..., 7.3740e-11,
          2.1114e-10, 2.0864e-10],
         [1.7335e-11, 1.3869e-10, 8.9176e-11,  ..., 4.6831e-11,
          2.5544e-12, 3.1659e-11]],

        [[7.2455e-12, 1.4813e-07, 1.0534e-07,  ..., 2.5493e-09,
          2.3771e-07, 2.8221e-07],
         [8.0982e-12, 7.1795e-10, 6.0842e-10,  ..., 5.6836e-10,
          3.2645e-09, 2.2962e-09],
         [2.6556e-11, 7.9651e-10, 1.0726e-09,  ..., 4.2068e-10,
          1.2215e-10, 5.3262e-10],
         [3.0790e-14, 1.0135e-10, 3.4097e-11,  ..., 6.1092e-12,
          7.1867e-11, 7.6840e-11],
         [7.4275e-12, 3.1673e-09, 5.3470e-11,  ..., 1.8163e-10,
          4.0547e-10, 2.6077e-10],
         [6.9171e-10, 2.2704e-10, 4.0006e-10,  ..., 3.1070e-09,
          1.4778e-11, 7.5035e-09]],

        ...,

        [[3.6378e-11, 5.5136e-08, 4.3467e-07,  ..., 1.1099e-08,
          9.0729e-07, 4.7979e-08],
         [3.8709e-13, 3.7642e-11, 1.9952e-10,  ..., 3.6484e-11,
          1.8407e-10, 1.7375e-10],
         [1.9826e-15, 1.1150e-10, 9.9602e-11,  ..., 6.2354e-12,
          3.7284e-10, 1.6273e-10],
         [4.6516e-13, 8.1765e-12, 1.8336e-11,  ..., 3.9129e-12,
          4.3517e-10, 1.7493e-11],
         [1.7726e-11, 1.2067e-10, 5.7707e-11,  ..., 3.8953e-12,
          7.4610e-10, 1.8609e-09],
         [2.4663e-12, 1.8448e-11, 9.0382e-12,  ..., 1.9733e-12,
          3.1279e-12, 4.1556e-12]],

        [[7.5203e-12, 3.5545e-07, 1.0302e-07,  ..., 3.4392e-08,
          1.5543e-07, 1.4889e-08],
         [1.0366e-11, 4.2307e-10, 3.9791e-10,  ..., 3.9466e-10,
          3.3194e-09, 2.4858e-10],
         [5.2874e-15, 2.3585e-10, 8.3765e-11,  ..., 2.9112e-11,
          9.7014e-10, 1.5476e-10],
         [3.3141e-12, 6.2255e-11, 4.3731e-11,  ..., 4.9656e-12,
          1.2393e-10, 1.1591e-11],
         [1.9290e-12, 2.0924e-10, 1.0868e-11,  ..., 1.2758e-11,
          8.7783e-11, 1.2862e-11],
         [1.9734e-11, 1.2031e-10, 1.5341e-10,  ..., 2.3305e-11,
          1.3316e-10, 2.0369e-10]],

        [[1.9581e-11, 1.6997e-07, 1.8873e-07,  ..., 1.9808e-08,
          2.6945e-07, 5.1306e-08],
         [1.0584e-12, 3.1926e-10, 6.7019e-10,  ..., 2.1223e-10,
          6.8825e-09, 6.2962e-10],
         [4.0771e-13, 1.3156e-10, 5.6477e-10,  ..., 2.9415e-11,
          2.6910e-11, 2.0029e-11],
         [1.5932e-13, 3.0206e-11, 6.5007e-12,  ..., 6.7845e-12,
          5.6351e-10, 7.8134e-11],
         [3.1644e-12, 1.2911e-10, 3.2278e-11,  ..., 2.5013e-11,
          2.1836e-10, 4.1814e-10],
         [2.1725e-11, 2.3066e-11, 2.8553e-11,  ..., 7.3589e-12,
          3.4484e-13, 2.7667e-11]]], device='cuda:0')
This is my alpha: 0.00017615971814445098
This is my mlm_output: MaskedLMOutput(loss=tensor(4.0244, device='cuda:0', grad_fn=<AddBackward0>), logits=tensor([[[-25.9538, -16.0904, -16.9402,  ..., -20.0059, -16.2852, -18.6348],
         [-20.8929, -15.4547, -14.1227,  ..., -14.8561, -13.0428, -15.1166],
         [-22.6841, -15.3722, -15.5586,  ..., -19.0142, -17.3503, -18.9108],
         [-24.6156, -15.0719, -17.7361,  ..., -18.9399, -15.0547, -16.4838],
         [-20.9998, -14.1055, -15.2173,  ..., -15.7790, -12.9358, -15.2316],
         [-17.7717, -13.1694, -14.0167,  ..., -17.8858, -13.5094, -14.8307]],

        [[-24.7852, -15.2070, -15.8833,  ..., -19.2476, -14.7530, -15.4475],
         [-19.5097, -14.5979, -13.9652,  ..., -14.7831, -12.7153, -13.8897],
         [-21.4651, -14.0881, -13.3835,  ..., -15.8800, -16.2162, -15.5784],
         [-20.2426, -16.1186, -17.1906,  ..., -17.3859, -13.3340, -14.7952],
         [-19.3170, -14.5669, -16.0779,  ..., -14.9236, -13.8477, -13.8128],
         [-12.7050,  -8.9836, -10.0641,  ..., -10.5624, -13.7037,  -9.9741]],

        [[-25.9689, -17.1327, -17.0647,  ..., -18.6419, -15.2281, -16.1439],
         [-17.6277, -13.8911, -13.3593,  ..., -13.5913, -11.4119, -11.9835],
         [-13.7871, -10.9770, -10.9427,  ..., -12.1039, -11.8724, -10.6759],
         [-23.7548, -16.2113, -16.6897,  ..., -18.3564, -15.8008, -15.5037],
         [-18.5622, -13.2427, -16.2547,  ..., -15.4660, -14.7422, -15.0274],
         [-12.1613, -10.9294, -11.2316,  ...,  -8.4510, -11.7232,  -7.3238]],

        ...,

        [[-25.1408, -17.0682, -15.7599,  ..., -19.1841, -14.8261, -16.6648],
         [-21.6670, -16.1962, -14.3909,  ..., -17.4561, -14.9937, -14.3364],
         [-26.5599, -13.7296, -15.4611,  ..., -17.0845, -13.5405, -13.9601],
         [-21.5976, -18.6392, -18.6659,  ..., -19.6747, -15.1278, -17.1086],
         [-18.1612, -13.3724, -14.9234,  ..., -16.0499, -12.3192, -11.0569],
         [-16.3946, -12.8122, -13.8082,  ..., -13.9240, -15.4160, -13.4523]],

        [[-26.5355, -15.0627, -15.3652,  ..., -17.3603, -15.2801, -16.7013],
         [-21.9856, -15.1095, -14.4487,  ..., -14.1956, -11.4445, -13.0965],
         [-23.1037, -12.5646, -13.0188,  ..., -13.8827, -11.6131, -13.1196],
         [-22.4346, -15.7287, -16.5498,  ..., -16.9934, -13.5240, -13.8969],
         [-19.9892, -12.2944, -14.9870,  ..., -16.7104, -13.9161, -15.4113],
         [-16.0915, -13.9262, -14.0230,  ..., -15.1312, -14.1622, -12.3746]],

        [[-24.8931, -15.8718, -15.8277,  ..., -19.0522, -14.7856, -15.7431],
         [-22.1628, -15.7167, -14.8589,  ..., -16.3529, -12.7367, -14.0223],
         [-21.2303, -14.7240, -14.3300,  ..., -17.8667, -16.9476, -17.8136],
         [-21.0658, -16.6612, -17.2124,  ..., -18.8320, -12.6268, -16.2500],
         [-19.7244, -14.7084, -16.4168,  ..., -16.4294, -13.8852, -13.6535],
         [-11.7181, -10.4268, -10.0929,  ..., -11.8916, -13.9373, -10.7553]]],
       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None) and loss_mlm is mlm_output.loss
This is my mrtd_labels: tensor([[-100,    0,    0,    0,    1,    0],
        [-100,    0,    0,    0,    0,    1],
        [-100,    0,    0,    0,    0,    1],
        [-100,    0,    0,    0,    0,    0],
        [-100,    0,    0,    1,    0,    0],
        [-100,    1,    0,    1,    0,    0],
        [-100,    0,    0,    0,    0,    0],
        [-100,    0,    0,    0,    0,    1],
        [-100,    0,    1,    1,    1,    0],
        [-100,    0,    0,    1,    1,    1]], device='cuda:0')
When it is output_MRTD: 


This is output_mrtd: BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.4689, -0.0344, -0.1410,  ...,  0.0301,  0.3944,  0.0498],
         [-0.1098, -0.2179, -0.1661,  ...,  0.3385,  0.3196, -0.3002],
         [-0.1472,  0.0081,  0.1053,  ..., -0.1534,  0.7069, -0.0371],
         [-0.4714, -0.2491, -0.5697,  ...,  0.2934, -0.0573,  1.0311],
         [ 0.8965, -0.0395,  0.1444,  ..., -0.3072,  0.3723, -0.2199],
         [ 1.0060,  0.1057, -0.2438,  ...,  0.2176,  0.4288,  0.2869]],

        [[ 0.2823, -0.3119,  0.0147,  ...,  0.1212,  0.1044, -0.0081],
         [-0.0324, -0.4452, -0.0328,  ...,  0.2117,  0.1895, -0.7279],
         [-0.3279, -0.1023, -0.2846,  ..., -0.0140,  0.4243, -0.2411],
         [-0.0604,  0.1402, -0.3967,  ...,  0.1048,  0.0270,  0.3564],
         [ 0.0851,  0.4854,  0.2793,  ..., -1.0489, -0.0457,  0.0376],
         [ 0.7121,  0.8522,  0.3008,  ..., -0.4121, -0.4347,  0.4389]],

        [[ 0.2803, -0.2954, -0.2112,  ...,  0.2409,  0.0498, -0.1637],
         [-0.3192, -0.0762, -0.2374,  ...,  0.2217,  0.4165, -0.5685],
         [-0.6442, -0.4898, -0.1441,  ...,  0.1407,  0.2478, -0.1844],
         [-0.9282, -0.7984, -0.2193,  ...,  0.0722,  0.1447,  0.6129],
         [-0.5625, -0.0639, -0.3313,  ...,  0.0494,  0.2057,  0.2981],
         [ 0.7886,  0.4971, -0.2515,  ..., -0.6418,  0.7807, -1.2099]],

        ...,

        [[ 0.1060,  0.0923, -0.2608,  ..., -0.0738,  0.1355, -0.1459],
         [-0.4526, -0.0198,  0.4234,  ...,  0.1717, -0.1364, -0.0172],
         [-0.9461, -0.8253, -0.2320,  ..., -0.1103,  0.4248, -0.0914],
         [-0.1281, -0.0584, -0.5390,  ...,  0.3206, -0.1404,  0.9909],
         [ 0.1506, -0.9072, -0.0905,  ..., -0.2216,  0.2228,  0.3800],
         [ 0.9106,  0.2143, -0.2636,  ...,  0.1314, -0.2708,  0.4289]],

        [[ 0.8185,  0.7442, -0.4807,  ...,  0.0847,  0.7409, -0.0809],
         [ 0.0021,  0.2220, -0.1033,  ...,  0.3677,  0.4092, -1.1501],
         [ 0.4180,  0.5818, -0.5182,  ...,  0.1022,  0.7593, -1.1916],
         [ 0.1218,  0.5684, -0.3332,  ...,  0.2597,  0.6985, -0.3213],
         [ 0.6029, -0.2021, -0.4316,  ...,  0.7323,  0.6256, -0.4843],
         [ 0.4464,  0.3205, -0.3894,  ..., -0.0764,  0.2309,  0.0379]],

        [[ 0.1030, -0.1106, -0.1088,  ...,  0.3014,  0.5287,  0.1328],
         [-0.8113, -0.0964, -0.1441,  ...,  0.5871, -0.0725, -0.3675],
         [-0.5408,  0.4211,  0.1092,  ...,  0.7599,  0.2745, -0.1751],
         [-0.8227, -0.0591, -1.1024,  ...,  0.3758,  0.5170,  0.9907],
         [-0.4297,  1.2461, -0.0120,  ...,  0.6738,  0.4769,  0.1222],
         [ 0.1601,  0.3995, -0.5052,  ...,  0.5922,  0.7469,  0.4030]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=None, hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None) 

This is my text1: {'input_ids': tensor([[ 101, 1996, 2158, 2003, 3061, 2007],
        [ 101, 1996, 2450, 2038, 2146, 2601],
        [ 101, 1996, 2611, 2003, 4147, 2014],
        [ 101, 2023, 2450, 2003, 4147, 1037],
        [ 101, 1037, 2158, 4147, 1037, 2317],
        [ 101, 1037, 2711, 2003, 3788, 2185],
        [ 101, 1996, 2158, 2003, 4147, 1037],
        [ 101, 2023, 2711, 2038, 2460, 2601],
        [ 101, 1996, 2711, 2003, 4147, 1037],
        [ 101, 1996, 2450, 2038, 2601, 2606]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1]], device='cuda:0')} 

This is my text1.attention_mask: tensor([[1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1]], device='cuda:0') 

This is my image_atts: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0') 

This is my image1 for image_embeds = self.visual_encoder(image1): tensor([[[[-1.4127e+00, -1.2959e+00, -1.2083e+00,  ...,  1.1128e+00,
            6.1648e-01,  1.4933e-01],
          [-1.4419e+00, -1.3105e+00, -1.2229e+00,  ...,  1.4924e+00,
            1.6968e+00,  1.8719e+00],
          [-1.4711e+00, -1.2667e+00, -1.2229e+00,  ..., -4.0541e-01,
           -3.0322e-01, -2.0103e-01],
          ...,
          [-8.8716e-01, -9.7475e-01, -9.0176e-01,  ..., -1.0915e+00,
           -8.8716e-01, -9.8935e-01],
          [-9.1636e-01, -9.3096e-01, -9.4555e-01,  ..., -1.0769e+00,
           -1.0331e+00, -1.0477e+00],
          [-8.8716e-01, -9.0176e-01, -9.0176e-01,  ..., -1.0039e+00,
           -9.8935e-01, -1.0039e+00]],

         [[-1.3769e+00, -1.2418e+00, -1.1518e+00,  ...,  1.2945e+00,
            7.6921e-01,  2.8896e-01],
          [-1.3919e+00, -1.2418e+00, -1.1518e+00,  ...,  1.7747e+00,
            1.9848e+00,  2.0749e+00],
          [-1.4069e+00, -1.1968e+00, -1.1518e+00,  ..., -1.3126e-01,
           -2.6204e-02,  6.3843e-02],
          ...,
          [-8.2162e-01, -9.1166e-01, -8.3662e-01,  ..., -1.0317e+00,
           -8.2162e-01, -9.2667e-01],
          [-8.5163e-01, -8.6664e-01, -8.8165e-01,  ..., -1.0167e+00,
           -9.7169e-01, -9.8670e-01],
          [-8.2162e-01, -8.3662e-01, -8.3662e-01,  ..., -9.4168e-01,
           -9.2667e-01, -9.4168e-01]],

         [[-1.1389e+00, -1.0110e+00, -9.3986e-01,  ...,  1.3922e+00,
            8.9453e-01,  4.3949e-01],
          [-1.1958e+00, -1.0536e+00, -9.6830e-01,  ...,  1.8188e+00,
            2.0179e+00,  2.1459e+00],
          [-1.2385e+00, -1.0536e+00, -1.0110e+00,  ..., -1.3329e-03,
            8.3987e-02,  1.8353e-01],
          ...,
          [-5.9858e-01, -6.8390e-01, -6.1280e-01,  ..., -7.9766e-01,
           -5.9858e-01, -6.9812e-01],
          [-6.2702e-01, -6.4124e-01, -6.5546e-01,  ..., -7.8344e-01,
           -7.4078e-01, -7.5500e-01],
          [-5.9858e-01, -6.1280e-01, -6.1280e-01,  ..., -7.1234e-01,
           -6.9812e-01, -7.1234e-01]]],


        [[[ 1.8865e+00,  1.9303e+00,  1.9303e+00,  ..., -2.3023e-01,
           -3.0322e-01, -3.4702e-01],
          [ 1.1274e+00,  1.1420e+00,  1.1274e+00,  ...,  3.8290e-01,
            5.1429e-01,  5.5808e-01],
          [ 4.2670e-01,  3.0991e-01,  1.7942e-02,  ...,  6.3108e-01,
            7.4786e-01,  7.6246e-01],
          ...,
          [ 1.0398e+00,  1.0398e+00,  1.0544e+00,  ...,  1.1420e+00,
            1.1566e+00,  1.1566e+00],
          [ 1.0836e+00,  1.0836e+00,  1.0836e+00,  ...,  1.1858e+00,
            1.2150e+00,  1.2150e+00],
          [ 1.1128e+00,  1.1128e+00,  1.0982e+00,  ...,  1.2150e+00,
            1.2150e+00,  1.2150e+00]],

         [[ 2.0749e+00,  2.0749e+00,  2.0749e+00,  ..., -2.6204e-02,
           -1.7628e-01, -2.8134e-01],
          [ 1.3245e+00,  1.3245e+00,  1.2645e+00,  ...,  1.0093e+00,
            1.2495e+00,  1.3245e+00],
          [ 3.3398e-01,  2.1392e-01,  3.8118e-03,  ...,  1.5346e+00,
            1.8198e+00,  1.8648e+00],
          ...,
          [ 9.4930e-01,  9.4930e-01,  9.7932e-01,  ...,  1.0844e+00,
            1.0994e+00,  1.0994e+00],
          [ 9.9432e-01,  9.9432e-01,  9.9432e-01,  ...,  1.1294e+00,
            1.1594e+00,  1.1594e+00],
          [ 1.0243e+00,  1.0243e+00,  1.0093e+00,  ...,  1.1594e+00,
            1.1594e+00,  1.1594e+00]],

         [[ 2.1459e+00,  2.1459e+00,  2.1459e+00,  ...,  2.6885e-01,
            1.6931e-01,  1.1243e-01],
          [ 1.1363e+00,  1.1363e+00,  1.1647e+00,  ...,  1.5060e+00,
            1.7193e+00,  1.7477e+00],
          [-1.1509e-01, -1.5775e-01, -2.8573e-01,  ...,  2.0606e+00,
            2.1459e+00,  2.1459e+00],
          ...,
          [ 6.5279e-01,  6.5279e-01,  6.8123e-01,  ...,  8.9453e-01,
            9.0875e-01,  9.0875e-01],
          [ 7.0967e-01,  7.0967e-01,  7.0967e-01,  ...,  9.3719e-01,
            9.6563e-01,  9.6563e-01],
          [ 7.3811e-01,  7.3811e-01,  7.5233e-01,  ...,  9.6563e-01,
            9.6563e-01,  9.6563e-01]]],


        [[[ 1.6968e+00,  1.6968e+00,  1.7114e+00,  ..., -5.5140e-01,
           -3.7622e-01, -5.0760e-01],
          [ 1.3318e+00,  1.3172e+00,  1.3026e+00,  ..., -8.2877e-01,
           -6.2439e-01, -3.9081e-01],
          [ 1.1566e+00,  1.1420e+00,  1.1420e+00,  ..., -7.4118e-01,
           -6.8278e-01, -1.4264e-01],
          ...,
          [ 9.0845e-01,  9.6684e-01,  9.8144e-01,  ..., -2.8862e-01,
           -3.4702e-01, -3.7622e-01],
          [ 8.0626e-01,  9.5224e-01,  9.6684e-01,  ..., -1.8644e-01,
           -2.3023e-01, -2.3023e-01],
          [ 5.2889e-01,  7.3327e-01,  8.5005e-01,  ..., -1.8644e-01,
           -1.8644e-01, -2.0103e-01]],

         [[ 2.0599e+00,  2.0599e+00,  2.0599e+00,  ..., -6.4152e-01,
           -5.5148e-01, -7.4658e-01],
          [ 2.0599e+00,  2.0299e+00,  1.9998e+00,  ..., -9.5669e-01,
           -7.9160e-01, -5.9650e-01],
          [ 2.0749e+00,  2.0599e+00,  2.0449e+00,  ..., -8.8165e-01,
           -8.3662e-01, -2.8134e-01],
          ...,
          [ 1.2495e+00,  1.2945e+00,  1.3095e+00,  ..., -4.0140e-01,
           -4.6143e-01, -4.9144e-01],
          [ 1.1444e+00,  1.2945e+00,  1.2945e+00,  ..., -2.9634e-01,
           -3.4137e-01, -3.4137e-01],
          [ 8.5925e-01,  1.0694e+00,  1.1744e+00,  ..., -2.9634e-01,
           -2.9634e-01, -3.1135e-01]],

         [[ 2.1175e+00,  2.1175e+00,  2.1175e+00,  ..., -6.8390e-01,
           -6.2702e-01, -8.4032e-01],
          [ 2.0606e+00,  2.0464e+00,  2.0321e+00,  ..., -9.1142e-01,
           -7.8344e-01, -5.9858e-01],
          [ 2.1175e+00,  2.0748e+00,  2.0606e+00,  ..., -8.2610e-01,
           -7.8344e-01, -2.7151e-01],
          ...,
          [ 1.4349e+00,  1.5202e+00,  1.5629e+00,  ..., -4.2793e-01,
           -4.8482e-01, -5.1326e-01],
          [ 1.3354e+00,  1.5060e+00,  1.5487e+00,  ..., -3.2839e-01,
           -3.7105e-01, -3.7105e-01],
          [ 1.0652e+00,  1.2927e+00,  1.4349e+00,  ..., -3.2839e-01,
           -3.2839e-01, -3.4261e-01]]],


        ...,


        [[[ 9.2304e-01,  9.6684e-01,  8.7925e-01,  ...,  1.9157e+00,
            1.9157e+00,  1.7844e+00],
          [ 7.4786e-01,  8.2086e-01,  7.1867e-01,  ...,  1.7990e+00,
            1.7552e+00,  1.4486e+00],
          [ 4.5590e-01,  5.5808e-01,  4.8509e-01,  ...,  1.4486e+00,
            1.5362e+00,  1.4340e+00],
          ...,
          [ 3.2451e-01,  3.2451e-01,  3.2451e-01,  ..., -1.8644e-01,
           -2.1563e-01, -2.4483e-01],
          [ 3.6830e-01,  3.6830e-01,  3.8290e-01,  ...,  3.5371e-01,
            3.8290e-01,  3.9750e-01],
          [ 4.2670e-01,  4.2670e-01,  4.4130e-01,  ...,  3.6830e-01,
            3.2451e-01,  7.6336e-02]],

         [[ 8.1423e-01,  8.5925e-01,  7.5420e-01,  ...,  1.9098e+00,
            1.9098e+00,  1.7747e+00],
          [ 6.3414e-01,  7.0918e-01,  6.0412e-01,  ...,  1.7747e+00,
            1.7297e+00,  1.4145e+00],
          [ 3.7901e-01,  4.6905e-01,  3.7901e-01,  ...,  1.4145e+00,
            1.5046e+00,  1.3995e+00],
          ...,
          [ 2.1392e-01,  2.1392e-01,  2.1392e-01,  ..., -2.6633e-01,
           -2.9634e-01, -3.2636e-01],
          [ 2.5894e-01,  2.5894e-01,  2.7395e-01,  ...,  2.1392e-01,
            2.4394e-01,  2.5894e-01],
          [ 3.1897e-01,  3.1897e-01,  3.3398e-01,  ...,  2.4394e-01,
            2.1392e-01, -4.1212e-02]],

         [[ 6.6701e-01,  7.2389e-01,  6.5279e-01,  ...,  1.7051e+00,
            1.7051e+00,  1.5771e+00],
          [ 4.3949e-01,  5.2481e-01,  4.5371e-01,  ...,  1.5771e+00,
            1.5344e+00,  1.2358e+00],
          [ 1.6931e-01,  2.5463e-01,  1.6931e-01,  ...,  1.2643e+00,
            1.3354e+00,  1.2358e+00],
          ...,
          [-1.4353e-01, -1.4353e-01, -1.4353e-01,  ..., -3.9949e-01,
           -4.2793e-01, -4.5637e-01],
          [-1.0087e-01, -1.0087e-01, -8.6653e-02,  ..., -1.1509e-01,
           -8.6653e-02, -7.2433e-02],
          [-4.3993e-02, -4.3993e-02, -2.9773e-02,  ..., -1.2931e-01,
           -1.8619e-01, -4.4215e-01]]],


        [[[-1.2804e-01, -1.2804e-01, -1.4264e-01,  ...,  7.6246e-01,
            7.7706e-01,  7.6246e-01],
          [-1.2804e-01, -1.2804e-01, -1.2804e-01,  ...,  6.8947e-01,
            7.4786e-01,  7.6246e-01],
          [-1.2804e-01, -1.2804e-01, -1.1344e-01,  ...,  6.4567e-01,
            7.1867e-01,  7.1867e-01],
          ...,
          [-4.4921e-01, -4.7840e-01, -4.3461e-01,  ..., -2.7403e-01,
           -2.0103e-01, -2.8862e-01],
          [-4.2001e-01, -4.0541e-01, -3.3242e-01,  ..., -1.8644e-01,
           -1.8644e-01, -4.0541e-01],
          [-2.1563e-01, -5.6599e-01, -4.4921e-01,  ..., -3.7622e-01,
           -4.2001e-01, -5.2220e-01]],

         [[ 4.8835e-02,  4.8835e-02,  3.3827e-02,  ...,  9.1929e-01,
            9.1929e-01,  9.0428e-01],
          [ 4.8835e-02,  4.8835e-02,  4.8835e-02,  ...,  8.5925e-01,
            9.0428e-01,  9.0428e-01],
          [ 4.8835e-02,  4.8835e-02,  6.3843e-02,  ...,  7.9922e-01,
            8.7426e-01,  8.5925e-01],
          ...,
          [-3.7138e-01, -4.0140e-01, -3.5637e-01,  ..., -1.4627e-01,object address  : 0x7f063f257a60
^CWARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers
--- Logging error ---
object refcount : 2
object type     : 0x873460
object type name: KeyboardInterrupt
object repr     : KeyboardInterrupt()
lost sys.stderr
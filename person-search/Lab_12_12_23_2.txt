         ...,
         [ 1.3923e-01,  6.0852e-02,  3.1412e-02,  ..., -4.1938e-02,
           3.8803e-02,  2.2349e-01],
         [ 1.2725e-01,  2.3326e-02,  3.4132e-01,  ...,  1.5550e-01,
           2.8311e-01,  3.7203e-01],
         [ 6.2186e-01, -2.8362e-01, -4.3563e-01,  ...,  7.0326e-01,
           5.4918e-01, -1.7531e-01]],

        [[ 1.4741e-01,  8.1096e-02,  4.5524e-02,  ...,  5.3079e-03,
          -2.8620e-02,  1.4171e-01],
         [ 2.9702e-01, -3.0456e-02,  1.8924e-01,  ...,  2.0146e-03,
           1.7437e-01,  2.6814e-01],
         [ 2.7170e-01,  1.1917e-01,  9.8633e-02,  ..., -1.6019e-02,
           1.1386e-02,  9.4164e-02],
         ...,
         [ 1.5928e-01, -4.7633e-03,  6.6486e-02,  ...,  8.1001e-03,
          -1.0321e-02,  2.0142e-01],
         [ 2.5482e-01,  3.5226e-02,  1.3333e-01,  ..., -6.8961e-02,
           6.0391e-02,  2.7085e-01],
         [ 2.3515e-01,  7.3106e-02,  1.4326e-01,  ..., -2.8747e-02,
           4.2281e-02,  1.8825e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
This is my image2 in image_embeds_m = self.visual_encoder_m(image2): tensor([[[[ 1.5362,  1.5216,  1.5070,  ...,  1.0398,  1.0982,  1.0836],
          [ 1.6530,  1.6384,  1.5946,  ...,  1.5070,  1.5216,  1.4632],
          [ 1.2296,  1.2296,  1.2296,  ...,  1.4048,  1.4194,  1.4048],
          ...,
          [ 0.2077,  0.1931,  0.1785,  ...,  1.1128,  1.1128,  1.0690],
          [ 0.1639,  0.1347,  0.1347,  ...,  1.0982,  1.0836,  1.0836],
          [ 0.1201,  0.1055,  0.1055,  ...,  1.0398,  1.0252,  1.0398]],

         [[ 1.5496,  1.5496,  1.5646,  ...,  1.1144,  1.1744,  1.1594],
          [ 1.3395,  1.3245,  1.2945,  ...,  1.1294,  1.1744,  1.1144],
          [ 1.2044,  1.2194,  1.2194,  ...,  0.8893,  0.8593,  0.8292],
          ...,
          [ 0.1689,  0.1389,  0.1239,  ...,  1.3395,  1.3095,  1.2495],
          [ 0.0789,  0.0488,  0.0488,  ...,  1.2945,  1.2795,  1.2795],
          [ 0.0338,  0.0188,  0.0188,  ...,  1.2194,  1.2044,  1.2194]],

         [[ 1.5344,  1.5487,  1.5913,  ...,  1.1789,  1.2358,  1.2216],
          [ 1.0083,  1.0225,  1.0083,  ...,  0.8234,  0.8234,  0.7381],
          [ 1.1505,  1.1505,  1.1647,  ...,  0.4679,  0.3542,  0.2688],
          ...,
          [ 0.1977,  0.1835,  0.1551,  ...,  1.4918,  1.4491,  1.3922],
          [ 0.0840,  0.0555,  0.0555,  ...,  1.4776,  1.4349,  1.4349],
          [ 0.0271,  0.0129,  0.0129,  ...,  1.4065,  1.3780,  1.3922]]],


        [[[ 0.3683,  0.3975,  0.4121,  ...,  1.8573,  1.7114,  1.7260],
          [ 0.3975,  0.3975,  0.3975,  ...,  1.9303,  1.9157,  1.7844],
          [ 0.4413,  0.4559,  0.4851,  ...,  1.3610,  1.3756,  1.3318],
          ...,
          [ 0.2077,  0.0763, -0.2010,  ..., -0.3908, -0.2448, -0.4200],
          [ 0.1055,  0.0909,  0.0763,  ..., -0.3908, -0.7266, -0.6390],
          [-0.0259,  0.0471,  0.0763,  ..., -0.6244, -0.5660, -0.4784]],

         [[ 0.6191,  0.6491,  0.6642,  ...,  2.0449,  1.9098,  1.9398],
          [ 0.6191,  0.6191,  0.6191,  ...,  2.0749,  2.0749,  2.0299],
          [ 0.6041,  0.6191,  0.6491,  ...,  1.5496,  1.5496,  1.5346],
          ...,
          [ 0.3040,  0.1689, -0.1163,  ..., -0.3114, -0.1613, -0.3414],
          [ 0.1989,  0.1839,  0.1689,  ..., -0.3114, -0.6565, -0.5665],
          [ 0.0638,  0.1389,  0.1689,  ..., -0.5515, -0.4914, -0.4014]],

         [[ 0.7381,  0.7808,  0.7950,  ...,  2.1175,  1.9753,  2.0037],
          [ 0.7523,  0.7523,  0.7523,  ...,  2.1459,  2.1459,  2.0748],
          [ 0.7666,  0.7808,  0.8092,  ...,  1.6340,  1.6482,  1.6198],
          ...,
          [ 0.4679,  0.3399,  0.0698,  ..., -0.1151,  0.0271, -0.1435],
          [ 0.3684,  0.3542,  0.3399,  ..., -0.1151, -0.4422, -0.3568],
          [ 0.2404,  0.3115,  0.3399,  ..., -0.3426, -0.2857, -0.2004]]],


        [[[-0.0405,  0.6165,  1.0690,  ...,  1.7114,  1.7260,  1.7260],
          [ 0.7187,  0.2077,  0.3829,  ...,  1.6968,  1.7260,  1.7552],
          [ 1.3318,  1.0106,  1.3172,  ...,  1.7406,  1.7406,  1.7552],
          ...,
          [ 0.5727,  0.5727,  0.5581,  ...,  0.4121,  0.4267,  0.4121],
          [ 0.4121,  0.4705,  0.4413,  ...,  0.3683,  0.4121,  0.4559],
          [ 0.3391,  0.3391,  0.2953,  ...,  0.2953,  0.3099,  0.3829]],

         [[ 0.1389,  0.8142,  1.2795,  ...,  1.9098,  1.8798,  1.8798],
          [ 0.7992,  0.2890,  0.5141,  ...,  1.8798,  1.8948,  1.8948],
          [ 1.4446,  1.0994,  1.4295,  ...,  1.9548,  1.9548,  1.9398],
          ...,
          [ 0.6942,  0.7092,  0.6942,  ...,  0.5441,  0.5591,  0.5441],
          [ 0.5141,  0.5891,  0.5741,  ...,  0.4841,  0.5291,  0.5741],
          [ 0.4390,  0.4390,  0.4240,  ...,  0.4090,  0.4240,  0.4991]],

         [[ 0.2973,  0.9088,  1.3638,  ...,  1.9326,  1.9753,  2.0179],
          [ 0.9230,  0.4395,  0.6528,  ...,  1.9468,  1.9468,  1.9610],
          [ 1.5629,  1.2358,  1.5487,  ...,  2.0606,  2.0321,  2.0179],
          ...,
          [ 0.8234,  0.8377,  0.8234,  ...,  0.6812,  0.6955,  0.6812],
          [ 0.6670,  0.7239,  0.7097,  ...,  0.6670,  0.7097,  0.7523],
          [ 0.5959,  0.5959,  0.5675,  ...,  0.5959,  0.6101,  0.6812]]],


        ...,


        [[[-1.5295, -1.5295, -1.5149,  ..., -1.0915, -1.0623, -1.0623],
          [-1.5149, -1.5149, -1.5149,  ..., -1.1061, -1.1061, -1.1061],
          [-1.5003, -1.5003, -1.5149,  ..., -1.1791, -1.1207, -1.0915],
          ...,
          [ 0.1785,  0.1785,  0.1931,  ...,  0.0909, -0.0988, -0.0988],
          [ 0.3829,  0.3829,  0.3829,  ..., -0.2156,  0.1201,  0.4413],
          [ 0.2515,  0.2515,  0.2369,  ...,  0.6603,  0.6895,  0.6749]],

         [[-1.4669, -1.4669, -1.4820,  ..., -0.9867, -0.9717, -0.9567],
          [-1.4820, -1.4820, -1.4820,  ..., -0.9567, -0.9567, -0.9567],
          [-1.4820, -1.4820, -1.4820,  ..., -1.0167, -0.9567, -0.9267],
          ...,
          [ 0.2289,  0.2289,  0.2289,  ...,  0.0488, -0.1463, -0.1463],
          [ 0.4090,  0.3940,  0.3940,  ..., -0.2663,  0.0789,  0.4090],
          [ 0.2589,  0.2439,  0.2439,  ...,  0.6341,  0.6642,  0.6491]],

         [[-1.1532, -1.1532, -1.1389,  ..., -0.8261, -0.8261, -0.8261],
          [-1.1674, -1.1532, -1.1389,  ..., -0.8261, -0.8261, -0.8261],
          [-1.1816, -1.1674, -1.1532,  ..., -0.8830, -0.8403, -0.8119],
          ...,
          [ 0.2831,  0.2973,  0.2973,  ...,  0.1266, -0.0582, -0.0582],
          [ 0.4110,  0.4110,  0.4110,  ..., -0.1720,  0.1551,  0.4679],
          [ 0.2546,  0.2404,  0.2404,  ...,  0.6812,  0.7097,  0.6955]]],


        [[[-0.3324, -0.4492, -0.8288,  ...,  1.9157,  1.8865,  1.9011],
          [-0.9310, -1.0915, -1.3689,  ...,  1.9157,  1.9011,  1.9157],
          [-1.2667, -1.4711, -1.5587,  ...,  1.9157,  1.9011,  1.9157],
          ...,
          [-0.6098, -0.0550,  1.0252,  ...,  1.2004,  1.2588,  1.2880],
          [-0.7704, -0.5660,  0.1785,  ...,  1.2150,  1.2296,  1.2734],
          [-1.0039, -0.7996, -0.6536,  ...,  1.1566,  1.2150,  1.1858]],

         [[-0.8366, -0.9117, -1.1518,  ...,  1.8948,  1.9098,  1.9398],
          [-1.2118, -1.2869, -1.4820,  ...,  1.8198,  1.8498,  1.8648],
          [-1.3469, -1.5420, -1.5720,  ...,  1.8198,  1.8047,  1.8198],
          ...,
          [-1.0317, -0.4764,  0.6491,  ...,  1.1294,  1.1894,  1.2344],
          [-1.0467, -0.9717, -0.2513,  ...,  1.0994,  1.1144,  1.1594],
          [-1.1968, -1.0767, -1.0617,  ...,  0.9493,  0.9643,  0.9043]],

         [[-0.9114, -0.9256, -1.1532,  ...,  1.4207,  1.4349,  1.4491],
          [-1.1958, -1.2100, -1.3522,  ...,  1.3922,  1.3922,  1.4065],
          [-1.2669, -1.3949, -1.3665,  ...,  1.3922,  1.3780,  1.3922],
          ...,
          [-1.1105, -0.7123,  0.2262,  ...,  1.0367,  1.0936,  1.1363],
          [-1.0536, -1.0963, -0.4990,  ...,  0.9656,  0.9799,  1.0225],
          [-1.1247, -1.0963, -1.1389,  ...,  0.8377,  0.8661,  0.8377]]],


        [[[-1.1645, -0.6098, -0.8288,  ..., -0.8288, -0.8288, -1.0039],
          [ 0.3829,  0.4267,  0.0617,  ..., -0.9456, -0.9893, -0.9893],
          [ 0.7479,  0.5143,  0.2953,  ..., -1.1499, -1.0477, -1.0769],
          ...,
          [ 0.0763,  0.1055,  0.1493,  ..., -0.0259,  0.0909,  0.1347],
          [ 0.0325,  0.0471,  0.0471,  ...,  0.0325,  0.0179,  0.0325],
          [ 0.0617,  0.0617,  0.0471,  ...,  0.2369,  0.2515,  0.2369]],

         [[-1.0017, -0.4614, -0.7466,  ..., -0.7616, -0.7616, -0.9417],
          [ 0.5441,  0.5441,  0.1239,  ..., -0.8816, -0.9267, -0.9267],
          [ 0.8442,  0.5741,  0.2589,  ..., -1.0918, -0.9867, -1.0167],
          ...,
          [ 0.1689,  0.1989,  0.2439,  ...,  0.1389,  0.2740,  0.3040],
          [ 0.1239,  0.1389,  0.1389,  ...,  0.1539,  0.1539,  0.1689],
          [ 0.1539,  0.1539,  0.1389,  ...,  0.3490,  0.3640,  0.3490]],

         [[-0.8119, -0.2857, -0.5275,  ..., -0.5417, -0.5417, -0.7123],
          [ 0.6812,  0.6812,  0.3115,  ..., -0.6555, -0.6981, -0.6981],
          [ 0.9941,  0.7381,  0.4679,  ..., -0.8545, -0.7550, -0.7834],
          ...,
          [ 0.3399,  0.3684,  0.4110,  ...,  0.2973,  0.4110,  0.4537],
          [ 0.2973,  0.3115,  0.3115,  ...,  0.3115,  0.3115,  0.3257],
          [ 0.3257,  0.3257,  0.3115,  ...,  0.4964,  0.5106,  0.4964]]]],
       device='cuda:0') and it shape is: torch.Size([10, 3, 50, 50])
This is my labels = input_ids.clone(): tensor([[ 101, 2023, 2158, 2038, 2460, 2304],
        [ 101, 1037, 2402, 2450, 2007, 2146],
        [ 101, 1037, 2158, 4147, 1037, 3756],
        [ 101, 2023, 2711, 2003, 3788, 2185],
        [ 101, 1996, 2611, 2038, 2304, 2606],
        [ 101, 1996, 3265, 3544, 2000, 2022],
        [ 101, 1037, 2450, 1999, 1037, 3756],
        [ 101, 1996, 2158, 2003, 4147, 1037],
        [ 101, 1996, 2158, 2003, 3788, 2002],
        [ 101, 1037, 2158, 1999, 1037, 2317]], device='cuda:0')
This is my  self.text_encoder.config.vocab_size: 30522
This is input_ids: tensor([[  101,  2023,  2158,  6179,  2460,  2304],
        [  101,   103,   103,  2450,  2007,  2146],
        [  101,  1037, 25243,  4147,  1037,  3756],
        [  101,  2023,  2711,  2003,  3788,  2185],
        [  101,  1996,  2611,   103,  2304,  2606],
        [  101,  1996,  3265,  3544,  2000,   103],
        [  101,  1037,  2450,  1999,  1037,  3756],
        [  101,  1996,   103,  2003,  4147,  1037],
        [  101,  1996,  2158,   103,  3788,  2002],
        [  101,  1037,  2158,  1999,  1037,  2317]], device='cuda:0')


This is targets=labels (format=(labels)): tensor([[-100, -100, -100, 2038, -100, -100],
        [-100, 1037, 2402, -100, -100, -100],
        [-100, -100, 2158, -100, -100, -100],
        [-100, -100, -100, -100, -100, -100],
        [-100, -100, -100, 2038, -100, -100],
        [-100, -100, -100, -100, -100, 2022],
        [-100, -100, -100, -100, -100, -100],
        [-100, -100, 2158, 2003, -100, -100],
        [-100, -100, -100, 2003, -100, -100],
        [-100, -100, -100, -100, -100, -100]], device='cuda:0')
When it is mlm output: 


This is mlm_output: MaskedLMOutput(loss=tensor(1.0236, device='cuda:0', grad_fn=<AddBackward0>), logits=tensor([[[-23.6054, -17.1349, -18.2568,  ..., -19.6016, -14.7323, -17.9380],
         [-21.5955, -15.9540, -15.6019,  ..., -17.9468, -16.8700, -15.5813],
         [-18.9572, -13.9240, -16.3031,  ..., -15.7551, -14.6509, -18.3812],
         [-13.7836, -10.3461, -12.4817,  ..., -12.4696,  -9.4171, -10.1708],
         [-14.6843, -11.0307, -14.9707,  ..., -14.9096, -11.1734, -13.0277],
         [-17.9005, -15.3278, -15.5215,  ..., -17.0850, -15.8403, -16.5397]],

        [[-23.8140, -16.6725, -14.7333,  ..., -18.3214, -14.4199, -15.9205],
         [-11.2205, -10.2673, -10.3836,  ..., -10.3359, -12.1935,  -9.1384],
         [-11.5605,  -8.9071, -11.8899,  ..., -11.6373, -10.9787,  -9.3745],
         [-18.6011, -13.8416, -14.1930,  ..., -15.1142, -15.1195, -15.3195],
         [-20.0832, -15.9697, -16.4600,  ..., -18.9863, -14.1686, -14.2886],
         [-14.7798, -12.5184, -13.9003,  ..., -13.2502, -14.5250, -11.3204]],

        [[-25.0229, -15.0822, -17.3323,  ..., -17.8694, -14.5702, -16.5168],
         [-21.6026, -16.1645, -17.5834,  ..., -18.6191, -14.6931, -16.1941],
         [-14.1769,  -9.2082,  -9.0723,  ..., -10.3396, -11.4268, -10.6443],
         [-18.6492, -11.3299, -15.3805,  ..., -15.6985, -14.9242, -15.5357],
         [-20.3585, -15.5977, -17.4572,  ..., -18.2798, -15.1871, -15.8536],
         [-19.8025, -13.5221, -15.0272,  ..., -13.2116, -14.1127, -15.6659]],

        ...,

        [[-24.1006, -15.7154, -17.1274,  ..., -19.5888, -14.2084, -15.9576],
         [-19.6080, -14.9073, -14.8095,  ..., -14.9660, -12.8923, -14.2151],
         [-14.2649,  -9.9608, -10.3081,  ..., -11.0015, -11.1926, -11.6552],
         [-25.0762, -16.0178, -16.6754,  ..., -18.2476, -15.8622, -17.6091],
         [-20.2417, -12.3566, -16.4162,  ..., -16.0199, -14.7580, -15.4249],
         [-16.2374, -14.4685, -15.2510,  ..., -16.2123, -15.9237, -13.6139]],

        [[-26.4946, -16.5884, -18.1355,  ..., -20.1552, -15.0985, -17.5243],
         [-19.8442, -14.9839, -14.2692,  ..., -15.6381, -12.7775, -14.1638],
         [-20.8583, -14.1143, -15.7008,  ..., -17.8464, -15.7428, -18.0337],
         [-15.3894,  -9.6829, -11.7433,  ..., -12.4108,  -9.2118, -10.6636],
         [-17.7792, -10.7304, -14.1450,  ..., -13.5720, -11.2448, -14.2158],
         [-10.9041, -12.0458, -12.0910,  ..., -12.0196, -11.1566, -12.8709]],

        [[-24.0182, -16.5805, -18.3573,  ..., -20.2139, -16.9763, -19.2358],
         [-21.5167, -16.9862, -18.1463,  ..., -19.3485, -14.4954, -16.6731],
         [-21.1850, -14.1907, -14.4890,  ..., -17.8341, -15.8780, -17.6016],
         [-20.3350, -14.6272, -15.6650,  ..., -18.6915, -12.6536, -16.5654],
         [-21.4186, -16.5099, -17.2765,  ..., -17.8504, -12.9104, -15.8136],
         [-18.5187, -15.8322, -14.5749,  ..., -15.7833, -16.3035, -16.8883]]],
       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None) 

This is my text1: {'input_ids': tensor([[ 101, 2023, 2158, 2038, 2460, 2304],
        [ 101, 1037, 2402, 2450, 2007, 2146],
        [ 101, 1037, 2158, 4147, 1037, 3756],
        [ 101, 2023, 2711, 2003, 3788, 2185],
        [ 101, 1996, 2611, 2038, 2304, 2606],
        [ 101, 1996, 3265, 3544, 2000, 2022],
        [ 101, 1037, 2450, 1999, 1037, 3756],
        [ 101, 1996, 2158, 2003, 4147, 1037],
        [ 101, 1996, 2158, 2003, 3788, 2002],
        [ 101, 1037, 2158, 1999, 1037, 2317]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1]], device='cuda:0')} 

This is my image_embeds: tensor([[[ 1.2234e-01,  7.2274e-02,  3.5909e-02,  ...,  1.2965e-03,
          -6.3463e-02,  2.1293e-01],
         [ 8.6368e-02, -7.2578e-02, -8.7895e-02,  ..., -6.7529e-02,
           6.4388e-02,  4.8792e-01],
         [ 3.7273e-01, -5.7743e-01,  2.0894e-01,  ..., -1.7550e-02,
           4.4978e-01, -4.7345e-01],
         ...,
         [ 3.4254e-01, -1.3181e-01, -5.1342e-01,  ..., -5.9612e-01,
           5.2664e-01, -6.5308e-01],
         [ 2.2554e-01,  3.4690e-01,  8.6488e-01,  ..., -3.6151e-01,
           2.6317e-01, -9.0375e-01],
         [ 9.7870e-02, -2.9610e-02,  6.0450e-02,  ..., -1.0963e-02,
          -7.3814e-02,  4.9580e-01]],

        [[ 1.3823e-01,  8.3950e-02,  4.6456e-02,  ...,  5.0254e-03,
          -1.5394e-02,  1.3656e-01],
         [ 1.3236e-01,  4.6976e-02,  4.9218e-02,  ...,  3.5199e-03,
           3.6297e-02,  1.6799e-01],
         [ 2.1077e-01,  2.2819e-02,  1.2437e-01,  ...,  6.4960e-02,
           6.4274e-02,  1.7410e-01],
         ...,
         [ 1.4029e-01,  5.3198e-02,  1.0101e-01,  ..., -4.6511e-02,
           7.2872e-02,  1.9513e-01],
         [ 1.7103e-01,  1.2275e-02,  7.5093e-02,  ...,  2.4499e-03,
           8.5960e-04,  1.9798e-01],
         [ 4.4925e-01, -3.4293e-01,  3.9694e-01,  ..., -7.8011e-02,
           4.2030e-01,  1.9188e-01]],

        [[ 1.0441e-01,  8.7098e-02,  2.9713e-02,  ..., -3.5644e-02,
          -5.3061e-02,  1.4849e-01],
         [ 2.9721e-01, -2.6726e-02,  2.8044e-01,  ...,  7.3634e-01,
           3.2337e-01,  2.2627e-01],
         [ 4.2587e-01, -6.3927e-01, -1.3118e-01,  ...,  5.5415e-01,
           1.0414e+00,  4.7044e-02],
         ...,
         [-2.7242e-02, -2.3438e-01, -1.4454e-01,  ..., -3.6122e-01,
          -1.8702e-01,  8.1292e-02],
         [ 1.7180e-01, -3.2484e-01,  8.0767e-02,  ...,  2.4777e-01,
           7.8556e-02,  1.0528e-01],
         [ 1.4114e-01,  8.6325e-02,  7.4413e-02,  ..., -1.1939e-01,
           1.0930e-02,  1.6665e-01]],

        ...,

        [[ 1.3837e-01,  7.1288e-02,  1.0589e-02,  ...,  7.9206e-03,
          -4.1331e-02,  1.4322e-01],
         [ 1.7431e-01, -5.6072e-03,  6.8357e-02,  ..., -9.9048e-03,
           1.0928e-02,  2.1025e-01],
         [ 1.7802e-01,  5.7445e-03,  3.3746e-02,  ...,  2.1101e-02,
          -1.8157e-02,  1.9875e-01],
         ...,
         [-6.9016e-02, -5.8801e-01,  3.1096e-02,  ..., -3.1477e-01,
           5.1281e-01, -1.6856e-01],
         [ 1.7307e-01,  5.2495e-02,  1.2749e-01,  ..., -7.0434e-03,
          -1.8128e-03,  1.3790e-01],
         [ 2.9955e-01,  6.1833e-02,  1.4868e-01,  ..., -8.1130e-02,
           1.5272e-01,  2.8142e-01]],

        [[ 1.2298e-01,  1.0896e-01,  8.3276e-03,  ...,  3.2079e-02,
          -5.4502e-02,  2.0600e-01],
         [ 1.3803e-01, -2.7296e-01,  1.2953e-01,  ...,  3.9883e-02,
           1.9737e-01,  5.4265e-01],
         [ 1.6503e-01, -7.9476e-01,  3.3880e-01,  ...,  3.9752e-01,
           1.3456e+00,  6.1626e-01],
         ...,
         [ 1.3923e-01,  6.0852e-02,  3.1412e-02,  ..., -4.1938e-02,
           3.8803e-02,  2.2349e-01],
         [ 1.2725e-01,  2.3326e-02,  3.4132e-01,  ...,  1.5550e-01,
           2.8311e-01,  3.7203e-01],
         [ 6.2186e-01, -2.8362e-01, -4.3563e-01,  ...,  7.0326e-01,
           5.4918e-01, -1.7531e-01]],

        [[ 1.4741e-01,  8.1096e-02,  4.5524e-02,  ...,  5.3079e-03,
          -2.8620e-02,  1.4171e-01],
         [ 2.9702e-01, -3.0456e-02,  1.8924e-01,  ...,  2.0146e-03,
           1.7437e-01,  2.6814e-01],
         [ 2.7170e-01,  1.1917e-01,  9.8633e-02,  ..., -1.6019e-02,
           1.1386e-02,  9.4164e-02],
         ...,
         [ 1.5928e-01, -4.7633e-03,  6.6486e-02,  ...,  8.1001e-03,
          -1.0321e-02,  2.0142e-01],
         [ 2.5482e-01,  3.5226e-02,  1.3333e-01,  ..., -6.8961e-02,
           6.0391e-02,  2.7085e-01],
         [ 2.3515e-01,  7.3106e-02,  1.4326e-01,  ..., -2.8747e-02,
           4.2281e-02,  1.8825e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)
This is my labels: tensor([[-100, -100, -100, 2038, -100, -100],
        [-100, 1037, 2402, -100, -100, -100],
        [-100, -100, 2158, -100, -100, -100],
        [-100, -100, -100, -100, -100, -100],
        [-100, -100, -100, 2038, -100, -100],
        [-100, -100, -100, -100, -100, 2022],
        [-100, -100, -100, -100, -100, -100],
        [-100, -100, 2158, 2003, -100, -100],
        [-100, -100, -100, 2003, -100, -100],
        [-100, -100, -100, -100, -100, -100]], device='cuda:0')
This is my prediction: tensor([[[1.4037e-10, 3.1760e-08, 2.3520e-08,  ..., 1.2138e-08,
          1.4466e-07, 6.6033e-09],
         [3.3978e-14, 1.9154e-11, 2.0633e-10,  ..., 2.6290e-12,
          5.5650e-11, 3.2876e-11],
         [9.0660e-13, 1.0888e-10, 3.4016e-11,  ..., 1.4746e-11,
          4.9957e-11, 4.9163e-13],
         [5.1349e-11, 8.4280e-10, 4.7369e-10,  ..., 1.6354e-10,
          3.9633e-09, 8.1625e-10],
         [2.1418e-10, 1.0470e-09, 1.3538e-10,  ..., 1.5422e-10,
          5.8903e-09, 3.7681e-10],
         [1.3338e-11, 3.4071e-10, 2.7139e-10,  ..., 4.0304e-11,
          7.8824e-10, 3.1965e-11]],

        [[2.9813e-11, 5.6653e-08, 4.1688e-08,  ..., 3.2699e-09,
          7.2315e-08, 9.3142e-08],
         [6.8706e-10, 1.1629e-09, 6.5414e-10,  ..., 7.6091e-10,
          2.0398e-10, 1.2536e-09],
         [6.8683e-10, 3.0151e-09, 3.5301e-10,  ..., 2.9022e-10,
          1.7718e-09, 2.2901e-09],
         [1.2413e-11, 6.1865e-10, 9.3408e-10,  ..., 2.8087e-10,
          1.9016e-10, 1.0317e-10],
         [7.7094e-13, 4.6976e-10, 1.1578e-10,  ..., 8.7277e-12,
          2.0272e-09, 4.4198e-10],
         [5.0935e-10, 2.3960e-09, 5.7495e-10,  ..., 1.4537e-09,
          6.2992e-10, 3.2069e-09]],

        [[5.7263e-11, 3.3518e-07, 7.0212e-09,  ..., 8.6036e-08,
          7.8346e-07, 2.9129e-08],
         [7.7163e-13, 2.2824e-10, 3.5383e-11,  ..., 1.2749e-11,
          1.8535e-09, 4.8829e-11],
         [5.3514e-12, 1.3262e-09, 9.5521e-10,  ..., 2.2508e-10,
          2.1648e-10, 1.2223e-10],
         [4.9618e-13, 2.0478e-09, 1.1033e-11,  ..., 1.5093e-11,
          8.5105e-11, 4.7740e-12],
         [1.9144e-12, 4.5497e-10, 2.9250e-11,  ..., 1.3498e-11,
          5.3738e-10, 8.4756e-11],
         [8.4468e-12, 8.9069e-10, 2.5900e-10,  ..., 1.6416e-09,
          2.2960e-10, 4.5120e-11]],

        ...,

        [[9.1099e-11, 2.8433e-07, 1.1455e-07,  ..., 3.9205e-08,
          1.2648e-06, 3.2874e-08],
         [5.1625e-12, 4.0314e-10, 2.0352e-10,  ..., 3.4782e-10,
          3.1196e-09, 6.2246e-10],
         [1.3360e-11, 2.4456e-10, 4.6466e-10,  ..., 1.1442e-10,
          1.1485e-10, 7.8904e-11],
         [3.1859e-14, 8.1547e-11, 2.9802e-11,  ..., 6.9103e-12,
          1.3782e-10, 2.0567e-11],
         [9.3407e-13, 5.5357e-10, 8.3244e-12,  ..., 3.6621e-11,
          1.2252e-10, 2.1684e-11],
         [1.1048e-10, 6.9315e-10, 2.4057e-10,  ..., 2.0213e-10,
          2.3279e-10, 1.2607e-09]],

        [[1.1468e-09, 1.3155e-06, 1.2597e-07,  ..., 7.9949e-08,
          7.9708e-06, 2.5261e-07],
         [2.1471e-12, 5.8572e-10, 6.8804e-10,  ..., 1.4101e-10,
          3.3805e-09, 7.3558e-10],
         [3.4797e-13, 1.3680e-10, 8.3437e-11,  ..., 9.4623e-12,
          7.0798e-11, 3.0969e-12],
         [8.4795e-12, 1.2953e-09, 1.0408e-10,  ..., 1.0597e-10,
          4.1342e-09, 3.9803e-10],
         [9.9530e-11, 8.7287e-09, 1.1361e-09,  ..., 2.9106e-09,
          7.1819e-09, 1.1673e-09],
         [9.5059e-10, 5.0911e-10, 3.2083e-10,  ..., 4.7415e-10,
          6.4648e-10, 1.0932e-10]],

        [[1.3252e-12, 3.4139e-08, 3.1354e-09,  ..., 1.9423e-10,
          1.1620e-07, 9.5687e-09],
         [1.1012e-13, 9.2776e-11, 8.6921e-12,  ..., 1.0993e-12,
          6.0078e-09, 3.7724e-11],
         [7.3679e-14, 2.4234e-10, 2.7923e-10,  ..., 5.6623e-12,
          8.7659e-11, 4.3649e-12],
         [2.5555e-13, 1.3206e-10, 3.8652e-11,  ..., 2.4182e-12,
          1.7902e-09, 3.3596e-11],
         [4.3138e-14, 2.5571e-11, 8.6743e-12,  ..., 1.6337e-12,
          5.1854e-10, 2.1754e-11],
         [1.6590e-12, 2.4576e-11, 1.5052e-10,  ..., 3.9317e-11,
          1.6563e-10, 4.5170e-12]]], device='cuda:0')
This is my alpha: 0.00011743981209630065
This is my mlm_output: MaskedLMOutput(loss=tensor(1.0236, device='cuda:0', grad_fn=<AddBackward0>), logits=tensor([[[-23.6054, -17.1349, -18.2568,  ..., -19.6016, -14.7323, -17.9380],
         [-21.5955, -15.9540, -15.6019,  ..., -17.9468, -16.8700, -15.5813],
         [-18.9572, -13.9240, -16.3031,  ..., -15.7551, -14.6509, -18.3812],
         [-13.7836, -10.3461, -12.4817,  ..., -12.4696,  -9.4171, -10.1708],
         [-14.6843, -11.0307, -14.9707,  ..., -14.9096, -11.1734, -13.0277],
         [-17.9005, -15.3278, -15.5215,  ..., -17.0850, -15.8403, -16.5397]],

        [[-23.8140, -16.6725, -14.7333,  ..., -18.3214, -14.4199, -15.9205],
         [-11.2205, -10.2673, -10.3836,  ..., -10.3359, -12.1935,  -9.1384],
         [-11.5605,  -8.9071, -11.8899,  ..., -11.6373, -10.9787,  -9.3745],
         [-18.6011, -13.8416, -14.1930,  ..., -15.1142, -15.1195, -15.3195],
         [-20.0832, -15.9697, -16.4600,  ..., -18.9863, -14.1686, -14.2886],
         [-14.7798, -12.5184, -13.9003,  ..., -13.2502, -14.5250, -11.3204]],

        [[-25.0229, -15.0822, -17.3323,  ..., -17.8694, -14.5702, -16.5168],
         [-21.6026, -16.1645, -17.5834,  ..., -18.6191, -14.6931, -16.1941],
         [-14.1769,  -9.2082,  -9.0723,  ..., -10.3396, -11.4268, -10.6443],
         [-18.6492, -11.3299, -15.3805,  ..., -15.6985, -14.9242, -15.5357],
         [-20.3585, -15.5977, -17.4572,  ..., -18.2798, -15.1871, -15.8536],
         [-19.8025, -13.5221, -15.0272,  ..., -13.2116, -14.1127, -15.6659]],

        ...,

        [[-24.1006, -15.7154, -17.1274,  ..., -19.5888, -14.2084, -15.9576],
         [-19.6080, -14.9073, -14.8095,  ..., -14.9660, -12.8923, -14.2151],
         [-14.2649,  -9.9608, -10.3081,  ..., -11.0015, -11.1926, -11.6552],
         [-25.0762, -16.0178, -16.6754,  ..., -18.2476, -15.8622, -17.6091],
         [-20.2417, -12.3566, -16.4162,  ..., -16.0199, -14.7580, -15.4249],
         [-16.2374, -14.4685, -15.2510,  ..., -16.2123, -15.9237, -13.6139]],

        [[-26.4946, -16.5884, -18.1355,  ..., -20.1552, -15.0985, -17.5243],
         [-19.8442, -14.9839, -14.2692,  ..., -15.6381, -12.7775, -14.1638],
         [-20.8583, -14.1143, -15.7008,  ..., -17.8464, -15.7428, -18.0337],
         [-15.3894,  -9.6829, -11.7433,  ..., -12.4108,  -9.2118, -10.6636],
         [-17.7792, -10.7304, -14.1450,  ..., -13.5720, -11.2448, -14.2158],
         [-10.9041, -12.0458, -12.0910,  ..., -12.0196, -11.1566, -12.8709]],

        [[-24.0182, -16.5805, -18.3573,  ..., -20.2139, -16.9763, -19.2358],
         [-21.5167, -16.9862, -18.1463,  ..., -19.3485, -14.4954, -16.6731],
         [-21.1850, -14.1907, -14.4890,  ..., -17.8341, -15.8780, -17.6016],
         [-20.3350, -14.6272, -15.6650,  ..., -18.6915, -12.6536, -16.5654],
         [-21.4186, -16.5099, -17.2765,  ..., -17.8504, -12.9104, -15.8136],
         [-18.5187, -15.8322, -14.5749,  ..., -15.7833, -16.3035, -16.8883]]],
       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None) and loss_mlm is mlm_output.loss
This is my mrtd_labels: tensor([[-100,    0,    1,    0,    1,    1],
        [-100,    0,    0,    0,    0,    0],
        [-100,    0,    1,    1,    0,    0],
        [-100,    0,    1,    0,    0,    0],
        [-100,    0,    0,    0,    0,    0],
        [-100,    0,    0,    0,    0,    0],
        [-100,    1,    1,    0,    0,    0],
        [-100,    0,    0,    0,    0,    0],
        [-100,    0,    0,    0,    0,    1],
        [-100,    0,    0,    0,    0,    0]], device='cuda:0')
When it is output_MRTD: 


This is output_mrtd: BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.5032, -0.0976, -0.1238,  ...,  0.2376, -0.0750,  0.1580],
         [-0.6999,  0.3206, -0.1182,  ...,  0.6240,  0.0785,  0.4533],
         [ 0.9236, -0.0388, -0.1245,  ...,  0.2860, -0.0679,  0.2320],
         [-0.5299, -0.1532, -0.7471,  ...,  0.7811, -0.4930,  0.4223],
         [-0.4202,  0.3356,  0.4701,  ..., -0.3566, -0.2521,  0.1974],
         [ 0.9320,  0.0900, -0.1481,  ..., -0.0519, -0.3301,  0.6490]],

        [[ 0.2552, -0.5206, -0.1408,  ..., -0.0277,  0.4750,  0.0579],
         [ 0.0664, -0.1611,  0.0149,  ..., -0.1315, -0.0336,  0.0016],
         [-0.5318, -0.2580,  0.2845,  ..., -0.4806,  0.4237, -1.2327],
         [-0.6597, -0.4323,  0.0564,  ...,  0.2450, -0.0245, -0.3198],
         [ 0.1336,  0.6316,  0.5343,  ..., -0.2061, -0.2225, -0.0765],
         [ 0.5346,  0.3500,  0.3892,  ..., -0.3435,  0.8541,  0.3121]],

        [[ 0.1934,  0.3416, -0.2553,  ...,  0.1563,  0.2392,  0.0620],
         [-0.3587,  0.0850, -0.4119,  ..., -0.0949, -0.1093, -0.1126],
         [ 1.0280,  0.2449,  0.3162,  ..., -0.1563,  0.9908, -0.1192],
         [-0.7337,  0.2051, -0.2630,  ...,  0.0259,  0.0522, -1.3479],
         [ 0.1066,  0.3528, -0.4948,  ...,  0.0655, -0.0875, -0.3663],
         [-0.0144,  0.4430,  0.4181,  ..., -0.0074,  0.3443, -0.1611]],

        ...,

        [[ 0.1390,  0.0104, -0.0187,  ...,  0.1880,  0.1963, -0.0759],
         [-0.3375,  0.1355, -0.2408,  ...,  0.3838,  0.3801, -0.5155],
         [-0.4276,  0.3878, -0.2038,  ..., -0.0442,  0.8022, -0.0451],
         [-0.6736, -0.5012, -0.1753,  ...,  0.1781,  0.1506,  0.3341],
         [-0.7818, -0.2731, -0.2328,  ...,  0.2726,  0.1476,  0.2214],
         [ 0.2741,  0.1020, -0.9247,  ...,  0.1434,  0.1795,  0.4274]],

        [[ 0.2713,  0.0402, -0.3673,  ...,  0.2402,  0.4389,  0.2120],
         [-0.0262,  0.1530, -0.1637,  ...,  0.6005,  0.0063, -0.4574],
         [-0.4942,  0.4720,  0.0467,  ...,  0.3427,  0.7066, -0.1341],
         [-0.1750,  0.0184, -0.1413,  ...,  0.7873,  0.1751,  0.7519],
         [ 0.3582, -0.2886,  0.1000,  ...,  0.5694,  0.2368,  0.1235],
         [ 1.0210,  0.1728, -0.2994,  ...,  0.2822,  0.5553, -0.0956]],

        [[ 0.2702, -0.2069, -0.2995,  ...,  0.1353,  0.3794, -0.1240],
         [-0.0526, -0.0428, -0.3434,  ...,  0.2506,  0.1491,  0.1281],
         [-0.5095,  0.4648, -0.4177,  ...,  0.0995,  0.7142, -0.0826],
         [-0.4162, -0.4287, -1.3689,  ...,  0.2886, -0.0928, -0.1524],
         [-0.1767, -0.2092, -0.4133,  ...,  0.2798,  0.3496,  0.2255],
         [ 0.6756, -0.1080, -0.9953,  ...,  0.1273,  0.9169,  0.2298]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=None, hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None) 

This is my text1: {'input_ids': tensor([[ 101, 2023, 2158, 2038, 2460, 2304],
        [ 101, 1037, 2402, 2450, 2007, 2146],
        [ 101, 1037, 2158, 4147, 1037, 3756],
        [ 101, 2023, 2711, 2003, 3788, 2185],
        [ 101, 1996, 2611, 2038, 2304, 2606],
        [ 101, 1996, 3265, 3544, 2000, 2022],
        [ 101, 1037, 2450, 1999, 1037, 3756],
        [ 101, 1996, 2158, 2003, 4147, 1037],
        [ 101, 1996, 2158, 2003, 3788, 2002],
        [ 101, 1037, 2158, 1999, 1037, 2317]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1]], device='cuda:0')} and its type: <class 'transformers.tokenization_utils_base.BatchEncoding'> 

This is my text1.attention_mask: tensor([[1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1]], device='cuda:0') and its type: <class 'torch.Tensor'> 

This is my image_atts: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0') and its type: <class 'torch.Tensor'>

This is my image1 for image_embeds = self.visual_encoder(image1): tensor([[[[-9.8935e-01, -9.7475e-01, -7.2658e-01,  ...,  1.4194e+00,
            1.4048e+00,  1.3902e+00],
          [-9.8845e-02, -1.2804e-01, -2.4483e-01,  ...,  1.5216e+00,
            1.5070e+00,  1.3756e+00],
          [ 3.3911e-01,  3.8290e-01,  1.9312e-01,  ...,  1.4778e+00,
            1.5946e+00,  1.5508e+00],
          ...,
          [-1.1255e-02,  4.2670e-01,  8.3545e-01,  ...,  1.0106e+00,
            1.0544e+00,  1.1128e+00],
          [ 3.5371e-01,  6.6027e-01,  8.9385e-01,  ...,  1.3756e+00,
            1.3756e+00,  1.3756e+00],
          [ 3.8290e-01,  6.8947e-01,  7.6246e-01,  ...,  1.3172e+00,
            1.3172e+00,  1.3172e+00]],

         [[-8.0661e-01, -7.7659e-01, -5.0645e-01,  ...,  6.6415e-01,
            4.8406e-01,  3.9401e-01],
          [-3.7138e-01, -3.4137e-01, -3.8639e-01,  ...,  7.6921e-01,
            6.0412e-01,  3.7901e-01],
          [-3.2636e-01, -1.9129e-01, -2.6633e-01,  ...,  7.2418e-01,
            6.9417e-01,  5.5910e-01],
          ...,
          [-2.0630e-01,  2.7395e-01,  7.5420e-01,  ...,  9.3429e-01,
            9.7932e-01,  1.0393e+00],
          [ 2.5894e-01,  5.8911e-01,  8.4425e-01,  ...,  1.3395e+00,
            1.3545e+00,  1.3545e+00],
          [ 3.4899e-01,  6.6415e-01,  7.5420e-01,  ...,  1.3245e+00,
            1.3245e+00,  1.3245e+00]],

         [[-3.5683e-01, -2.9995e-01, -1.5553e-02,  ...,  1.3496e+00,
            1.3638e+00,  1.3780e+00],
          [-2.4307e-01, -1.7197e-01, -1.2931e-01,  ...,  1.4491e+00,
            1.4776e+00,  1.3638e+00],
          [-3.9949e-01, -2.2885e-01, -2.0041e-01,  ...,  1.4065e+00,
            1.5629e+00,  1.5344e+00],
          ...,
          [-1.0087e-01,  3.5417e-01,  7.9499e-01,  ...,  9.5141e-01,
            9.9407e-01,  1.0510e+00],
          [ 3.3995e-01,  6.5279e-01,  8.9453e-01,  ...,  1.3354e+00,
            1.3354e+00,  1.3354e+00],
          [ 4.5371e-01,  7.5233e-01,  8.2343e-01,  ...,  1.3069e+00,
            1.3069e+00,  1.3069e+00]]],


        [[[-5.5140e-01,  3.8290e-01,  7.4786e-01,  ...,  8.2086e-01,
            7.9166e-01,  3.6830e-01],
          [-2.7403e-01,  5.7268e-01,  6.4567e-01,  ...,  6.1648e-01,
            4.2670e-01,  4.7139e-02],
          [ 9.0935e-02,  6.7487e-01,  7.0407e-01,  ...,  6.6027e-01,
            2.5152e-01, -1.4264e-01],
          ...,
          [-5.0760e-01,  1.7853e-01,  3.8290e-01,  ..., -9.8935e-01,
           -4.0451e-02, -3.3242e-01],
          [-3.4702e-01, -2.1563e-01, -2.0103e-01,  ..., -3.4702e-01,
            1.6393e-01, -6.6818e-01],
          [ 2.3692e-01,  3.2451e-01,  3.0991e-01,  ...,  3.3439e-03,
            7.6336e-02, -8.8716e-01]],

         [[-8.3662e-01,  7.8851e-02,  4.0902e-01,  ...,  3.7901e-01,
            3.4899e-01, -8.6235e-02],
          [-5.5148e-01,  2.7395e-01,  3.0397e-01,  ...,  1.9891e-01,
            3.8118e-03, -4.0140e-01],
          [-1.9129e-01,  3.7901e-01,  3.7901e-01,  ...,  2.4394e-01,
           -1.7628e-01, -5.8149e-01],
          ...,
          [-5.5148e-01,  1.5389e-01,  3.3398e-01,  ..., -9.4168e-01,
            1.8820e-02, -2.8134e-01],
          [-3.1135e-01, -1.9129e-01, -2.2130e-01,  ..., -2.8134e-01,
            2.4394e-01, -6.2651e-01],
          [ 3.3398e-01,  4.0902e-01,  3.3398e-01,  ...,  7.8851e-02,
            1.3888e-01, -8.5163e-01]],

         [[-6.4124e-01,  2.4041e-01,  5.2481e-01,  ...,  3.6839e-01,
            3.1151e-01, -1.1509e-01],
          [-3.7105e-01,  4.1105e-01,  4.2527e-01,  ...,  2.2619e-01,
            4.1327e-02, -3.5683e-01],
          [-1.5553e-02,  5.1059e-01,  4.9637e-01,  ...,  3.6839e-01,
           -5.8213e-02, -4.5637e-01],
          ...,
          [-3.4261e-01,  3.1151e-01,  4.6793e-01,  ..., -5.9858e-01,
            3.3995e-01,  6.9767e-02],
          [-1.5775e-01, -5.8213e-02, -7.2433e-02,  ...,  2.7107e-02,
            5.3903e-01, -2.5729e-01],
          [ 4.3949e-01,  4.9637e-01,  4.3949e-01,  ...,  3.5417e-01,
            4.5371e-01, -4.5637e-01]]],


        [[[ 3.8290e-01,  1.7853e-01,  1.4933e-01,  ..., -1.1344e-01,
           -8.4247e-02, -4.0451e-02],
          [-7.5577e-01, -5.3680e-01, -2.7403e-01,  ..., -7.5577e-01,
           -7.1198e-01, -7.5577e-01],
          [-8.5796e-01, -7.7037e-01, -8.8716e-01,  ..., -1.0477e+00,
           -9.4555e-01, -9.3096e-01],
          ...,
          [ 6.4567e-01,  6.1648e-01,  6.0188e-01,  ...,  2.0772e-01,
            1.9312e-01,  2.2232e-01],
          [ 6.1648e-01,  6.1648e-01,  5.7268e-01,  ...,  3.8290e-01,
            3.9750e-01,  4.2670e-01],
          [ 3.3911e-01,  3.5371e-01,  3.6830e-01,  ...,  2.8071e-01,
            2.3692e-01,  2.3692e-01]],

         [[ 6.7916e-01,  4.5404e-01,  4.3904e-01,  ...,  1.2387e-01,
            1.0887e-01,  1.3888e-01],
          [-5.9650e-01, -3.7138e-01, -1.1625e-01,  ..., -5.6648e-01,
           -5.6648e-01, -6.4152e-01],
          [-8.0661e-01, -7.1656e-01, -8.5163e-01,  ..., -9.4168e-01,
           -8.5163e-01, -8.3662e-01],
          ...,
          [ 6.7916e-01,  6.4915e-01,  6.3414e-01,  ...,  3.0397e-01,
            2.8896e-01,  3.1897e-01],
          [ 6.9417e-01,  6.7916e-01,  6.4915e-01,  ...,  4.8406e-01,
            4.9907e-01,  5.2908e-01],
          [ 4.2403e-01,  4.3904e-01,  4.5404e-01,  ...,  3.7901e-01,
            3.3398e-01,  3.3398e-01]],

         [[ 3.5417e-01,  1.5509e-01,  1.4087e-01,  ..., -2.1463e-01,
           -2.1463e-01, -1.8619e-01],
          [-6.6968e-01, -4.5637e-01, -2.0041e-01,  ..., -7.2656e-01,
           -7.1234e-01, -7.5500e-01],
          [-6.6968e-01, -5.8436e-01, -7.1234e-01,  ..., -8.5454e-01,
           -7.5500e-01, -7.4078e-01],
          ...,
          [ 7.9499e-01,  7.6655e-01,  7.3811e-01,  ...,  4.3949e-01,
            4.2527e-01,  4.5371e-01],
          [ 8.2343e-01,  8.0921e-01,  7.8077e-01,  ...,  6.1013e-01,
            6.2435e-01,  6.5279e-01],
          [ 5.5325e-01,  5.6747e-01,  5.8169e-01,  ...,  5.1059e-01,
            4.6793e-01,  4.6793e-01]]],


        ...,


        [[[-9.4555e-01, -9.4555e-01, -9.4555e-01,  ..., -9.8935e-01,
           -9.7475e-01, -9.8935e-01],
          [-9.4555e-01, -9.4555e-01, -9.4555e-01,  ..., -1.0623e+00,
           -9.8935e-01, -9.8935e-01],
          [-9.4555e-01, -9.4555e-01, -9.4555e-01,  ..., -1.1645e+00,
           -1.0623e+00, -1.0039e+00],
          ...,
          [ 4.2670e-01,  3.5371e-01,  3.5371e-01,  ..., -1.4264e-01,
           -2.8862e-01, -3.0322e-01],
          [ 3.8290e-01,  4.1210e-01,  4.1210e-01,  ...,  1.7942e-02,
            3.2541e-02,  1.4933e-01],
          [ 4.5590e-01,  4.9969e-01,  4.8509e-01,  ...,  1.4933e-01,
            2.9531e-01,  3.8290e-01]],

         [[-9.4168e-01, -9.4168e-01, -9.4168e-01,  ..., -9.4168e-01,
           -9.4168e-01, -9.4168e-01],
          [-9.4168e-01, -9.4168e-01, -9.4168e-01,  ..., -1.0017e+00,
           -9.2667e-01, -9.4168e-01],
          [-9.4168e-01, -9.4168e-01, -9.4168e-01,  ..., -1.0767e+00,
           -1.0017e+00, -9.5669e-01],
          ...,
          [ 5.2908e-01,  4.5404e-01,  4.5404e-01,  ..., -5.6219e-02,
           -2.0630e-01, -2.2130e-01],
          [ 4.8406e-01,  5.1408e-01,  5.1408e-01,  ...,  1.0887e-01,
            1.2387e-01,  2.4394e-01],
          [ 5.5910e-01,  6.0412e-01,  5.8911e-01,  ...,  2.4394e-01,
            3.9401e-01,  4.8406e-01]],

         [[-7.8344e-01, -7.8344e-01, -7.8344e-01,  ..., -7.6922e-01,
           -7.6922e-01, -7.9766e-01],
          [-7.8344e-01, -7.8344e-01, -7.8344e-01,  ..., -8.2610e-01,
           -7.6922e-01, -7.9766e-01],
          [-7.8344e-01, -7.8344e-01, -7.8344e-01,  ..., -9.3986e-01,
           -8.5454e-01, -8.2610e-01],
          ...,
          [ 6.8123e-01,  6.1013e-01,  6.1013e-01,  ...,  1.2665e-01,
           -1.5553e-02, -2.9773e-02],
          [ 6.3857e-01,  6.6701e-01,  6.6701e-01,  ...,  2.8307e-01,
            2.9729e-01,  4.1105e-01],
          [ 7.0967e-01,  7.5233e-01,  7.3811e-01,  ...,  4.1105e-01,
            5.5325e-01,  6.3857e-01]]],


        [[[-2.3023e-01, -6.9648e-02,  4.7139e-02,  ...,  3.5371e-01,
           -6.9648e-02, -2.1563e-01],
          [-1.2804e-01, -6.9648e-02,  1.7942e-02,  ..., -1.2804e-01,
           -6.9648e-02, -5.5050e-02],
          [ 1.5508e+00,  1.6822e+00,  1.5800e+00,  ..., -6.2439e-01,
           -5.8059e-01, -5.8059e-01],
          ...,
          [-8.4247e-02, -6.9648e-02,  4.7139e-02,  ...,  7.0407e-01,
            7.1867e-01,  7.6246e-01],
          [ 1.2013e-01,  1.6393e-01,  2.0772e-01,  ...,  7.1867e-01,
            7.3327e-01,  8.6465e-01],
          [ 2.3692e-01,  2.3692e-01,  2.8071e-01,  ...,  7.3327e-01,
            7.9166e-01,  8.7925e-01]],

         [[-5.6219e-02,  9.3858e-02,  1.8391e-01,  ...,  4.6905e-01,
            1.8820e-02, -1.6127e-01],
          [ 3.3398e-01,  3.6400e-01,  4.0902e-01,  ..., -1.1196e-02,
            4.8835e-02,  6.3843e-02],
          [ 1.9098e+00,  2.0149e+00,  1.9848e+00,  ..., -5.5148e-01,
           -5.2146e-01, -5.6648e-01],
          ...,
          [-1.9129e-01, -1.7628e-01, -5.6219e-02,  ...,  5.5910e-01,
            5.7411e-01,  6.1913e-01],
          [ 1.8820e-02,  6.3843e-02,  1.2387e-01,  ...,  5.7411e-01,
            5.8911e-01,  7.2418e-01],
          [ 9.3858e-02,  1.0887e-01,  1.3888e-01,  ...,  5.8911e-01,
            6.4915e-01,  7.3919e-01]],

         [[ 1.5509e-01, -2.9773e-02, -1.3329e-03,  ...,  4.5371e-01,
            2.7107e-02, -1.2931e-01],
          [ 1.0510e+00,  1.0083e+00,  1.0367e+00,  ..., -8.6653e-02,
           -5.8213e-02, -1.5553e-02],
          [ 1.9895e+00,  2.1032e+00,  2.0606e+00,  ..., -5.1326e-01,
           -5.4170e-01, -5.4170e-01],
          ...,
          [-2.4307e-01, -2.2885e-01, -1.1509e-01,  ...,  4.5371e-01,
            4.6793e-01,  5.1059e-01],
          [-8.6653e-02, -4.3993e-02,  1.2887e-02,  ...,  4.6793e-01,
            4.8215e-01,  6.1013e-01],
          [-2.9773e-02, -2.9773e-02,  1.2887e-02,  ...,  4.5371e-01,
            5.2481e-01,  6.1013e-01]]],


        [[[-6.9648e-02, -2.5853e-02, -6.9648e-02,  ...,  3.9750e-01,
            3.9750e-01,  3.9750e-01],
          [-9.8845e-02, -6.9648e-02, -1.2804e-01,  ...,  4.7049e-01,
            4.7049e-01,  4.7049e-01],
          [-5.5050e-02, -6.9648e-02, -1.5724e-01,  ...,  3.3911e-01,
            3.2451e-01,  3.0991e-01],
          ...,
          [ 1.7260e+00,  1.7260e+00,  1.7260e+00,  ...,  1.8281e+00,
            1.8281e+00,  1.8427e+00],
          [ 1.5216e+00,  1.5362e+00,  1.5654e+00,  ...,  1.8719e+00,
            1.8719e+00,  1.8719e+00],
          [ 1.1858e+00,  1.2004e+00,  1.2442e+00,  ...,  1.9157e+00,
            1.9157e+00,  1.9011e+00]],

         [[-1.1625e-01, -7.1227e-02, -1.1625e-01,  ...,  4.2403e-01,
            4.2403e-01,  4.2403e-01],
          [-1.4627e-01, -1.1625e-01, -1.7628e-01,  ...,  4.9907e-01,
            4.9907e-01,  4.9907e-01],
          [-1.1625e-01, -1.3126e-01, -2.2130e-01,  ...,  3.6400e-01,
            3.4899e-01,  3.3398e-01],
          ...,
          [ 1.8498e+00,  1.8498e+00,  1.8498e+00,  ...,  1.9698e+00,
            1.9698e+00,  1.9848e+00],
          [ 1.6247e+00,  1.6397e+00,  1.6697e+00,  ...,  2.0149e+00,
            2.0149e+00,  2.0149e+00],
          [ 1.2645e+00,  1.2795e+00,  1.3245e+00,  ...,  2.0599e+00,
            2.0599e+00,  2.0449e+00]],

         [[ 8.3987e-02,  1.2665e-01,  8.3987e-02,  ...,  6.8123e-01,
            6.6701e-01,  6.6701e-01],
          [ 5.5547e-02,  8.3987e-02,  2.7107e-02,  ...,  7.3811e-01,
            7.2389e-01,  7.0967e-01],
          [ 8.3987e-02,  5.5547e-02, -1.3329e-03,  ...,  5.8169e-01,
            5.5325e-01,  5.5325e-01],
          ...,
          [ 2.0321e+00,  2.0179e+00,  2.0037e+00,  ...,  2.0464e+00,
            2.0464e+00,  2.0606e+00],
          [ 1.8899e+00,  1.8899e+00,  1.9184e+00,  ...,  2.0890e+00,
            2.0890e+00,  2.0890e+00],
          [ 1.6055e+00,  1.6198e+00,  1.6624e+00,  ...,  2.1317e+00,
            2.1317e+00,  2.1175e+00]]]], device='cuda:0') and it got shape like: torch.Size([10, 3, 50, 50])
This is my image1.type: <class 'torch.Tensor'>
This is my image_embeds: tensor([[[ 1.6812e-01,  7.7459e-02,  4.2383e-02,  ...,  2.7137e-02,
          -3.3812e-02,  1.1747e-01],
         [ 1.7945e-01,  1.1131e-01,  1.7084e-01,  ..., -6.0692e-02,
           3.1632e-01,  3.5516e-01],
         [ 2.3307e-01, -1.7064e-02,  1.4258e-01,  ..., -2.4142e-02,
           2.7814e-02,  1.5903e-01],
         ...,
         [ 1.6892e-01,  1.4093e-01,  1.7248e-01,  ..., -2.5796e-02,
           2.6723e-02,  2.1252e-01],
         [ 2.4448e-01,  5.5012e-02,  1.1823e-01,  ...,  2.4236e-02,
          -5.8076e-03,  1.4984e-01],
         [ 2.0357e-01,  6.9421e-02,  9.1794e-02,  ...,  2.4135e-02,
          -1.5578e-02,  1.5615e-01]],

        [[ 1.5092e-01,  9.2454e-02,  3.2357e-02,  ...,  4.3368e-03,
          -3.1892e-02,  1.3314e-01],
         [ 1.3152e-01, -1.4082e-01, -4.6497e-02,  ..., -2.1709e-01,
           1.3172e+00, -4.6869e-01],
         [ 1.9281e-01,  2.6161e-02,  1.0753e-01,  ...,  1.8374e-03,
           3.1788e-02,  1.6044e-01],
         ...,
         [ 1.4720e-01,  6.9640e-02,  7.2628e-02,  ...,  9.1778e-04,
          -1.3090e-02,  2.0855e-01],
         [ 2.0165e-01,  7.8447e-02,  8.2403e-02,  ..., -1.6394e-02,
           1.4769e-02,  1.8123e-01],
         [ 1.5272e-01,  2.2240e-02,  1.0875e-01,  ..., -4.2184e-03,
          -1.5018e-02,  1.3871e-01]],

        [[ 1.1367e-01,  9.7785e-02,  1.7522e-02,  ...,  1.2276e-02,
          -3.2256e-02,  1.4261e-01],
         [ 1.6653e-01,  1.0670e-01,  7.7512e-02,  ..., -8.1035e-02,
           7.5472e-02,  2.0185e-01],
         [-5.8466e-01, -1.8388e-01, -1.5106e-01,  ...,  2.0084e-01,
           7.7185e-01,  9.3203e-02],
         ...,
         [ 2.2931e-01,  7.2640e-02,  3.7813e-03,  ..., -1.1247e-01,
           4.7151e-02,  2.5653e-01],
         [ 1.6135e-01,  8.1744e-02,  1.0226e-01,  ..., -6.1256e-02,
           1.0100e-01,  1.8936e-01],
         [ 5.3249e-01, -4.9593e-01, -2.0276e-01,  ..., -1.4938e-01,
           6.7260e-01, -1.4676e-02]],

        ...,

        [[ 1.5508e-01,  9.8143e-02,  3.9238e-02,  ..., -1.3610e-02,
          -2.0065e-02,  1.2937e-01],
         [ 1.9160e-01,  4.0594e-02,  2.7191e-02,  ...,  3.0124e-02,
          -2.7034e-03,  2.1502e-01],
         [ 2.1747e-01,  5.7696e-02,  9.6475e-02,  ...,  1.3509e-02,
           3.0390e-02,  1.5720e-01],
         ...,
         [ 1.6883e-01,  6.0633e-02,  9.2827e-02,  ...,  8.7872e-04,
           1.1276e-02,  1.5155e-01],
         [ 3.5077e-01,  7.3602e-02,  1.5864e-01,  ..., -1.5367e-03,
           1.2103e-01,  1.4023e-02],
         [ 1.8802e-01,  7.1971e-02,  7.6704e-02,  ..., -2.0532e-02,
           4.0548e-03,  1.1993e-01]],

        [[ 1.1738e-01,  9.3652e-02,  1.9649e-02,  ...,  2.7031e-02,
          -2.5312e-02,  1.6107e-01],
         [ 1.2900e-01,  1.3290e-01,  1.2464e-01,  ..., -2.4215e-02,
           3.5202e-02,  1.4839e-01],
         [-9.7341e-01, -7.3717e-01, -2.4112e-01,  ...,  9.7872e-03,
           1.2036e+00, -2.0142e-01],
         ...,
         [-1.0188e-01, -1.3913e-03, -2.0646e-03,  ..., -2.4479e-01,
           1.1105e-01,  3.4958e-01],
         [ 1.0972e-01,  7.4368e-02,  6.0495e-02,  ..., -1.3778e-01,
           1.5279e-01,  2.6887e-01],
         [ 1.3702e-01,  5.5880e-02,  8.6597e-02,  ...,  3.0532e-02,
          -2.7090e-02,  1.9995e-01]],

        [[ 1.5121e-01,  9.9012e-02,  5.0330e-02,  ..., -1.5221e-02,
          -1.9118e-02,  1.5139e-01],
         [ 1.4063e-01,  7.1586e-02,  1.6257e-01,  ...,  1.1827e-03,
           9.3583e-02,  1.7900e-01],
         [ 1.4929e-01,  4.7819e-02,  8.7260e-02,  ..., -6.9242e-03,
           5.6560e-02,  1.9101e-01],
         ...,
         [ 1.7614e-01,  6.3768e-02,  3.6941e-02,  ...,  5.1652e-03,
          -6.3345e-03,  2.4096e-01],
         [ 1.7528e-01,  4.9959e-02,  1.5250e-01,  ..., -1.0962e-02,
           2.1870e-02,  2.1696e-01],
         [ 1.8597e-01,  1.3460e-01,  1.4943e-01,  ...,  5.0404e-03,
          -3.7696e-03,  1.7378e-01]]], device='cuda:0',
       grad_fn=<NativeLayerNormBackward0>)